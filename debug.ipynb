{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d77d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/fast/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25be6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25eaa7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.51it/s]\n"
     ]
    }
   ],
   "source": [
    "lm_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "lm_model = lm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac350d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lm_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9958d813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850a6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = inputs[\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df1ecbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646,   9707,   1879]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323a007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "    (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in range(28):\n",
    "    print(lm_model.model.layers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64387caa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2Attention' object has no attribute 'rotary_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2Attention' object has no attribute 'rotary_emb'"
     ]
    }
   ],
   "source": [
    "print(lm_model.model.layers[0].self_attn.rotary_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cbc14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 3, 3584])\n",
      "Layer 0 shape: torch.Size([3, 3584])\n",
      "Layer 15 shape: torch.Size([3, 3584])\n",
      "Layer 27 shape: torch.Size([3, 3584])\n",
      "\n",
      "All captured activations:\n",
      "embeddings: torch.Size([1, 3, 3584])\n",
      "layer_0_input_layernorm: torch.Size([3, 3584])\n",
      "layer_0_q_proj: torch.Size([3, 3584])\n",
      "layer_0_k_proj: torch.Size([3, 512])\n",
      "layer_0_v_proj: torch.Size([3, 512])\n",
      "layer_0_attention: torch.Size([1, 3, 3584])\n",
      "layer_0: torch.Size([1, 3, 3584])\n",
      "layer_1_input_layernorm: torch.Size([3, 3584])\n",
      "layer_1_q_proj: torch.Size([3, 3584])\n",
      "layer_1_k_proj: torch.Size([3, 512])\n",
      "layer_1_v_proj: torch.Size([3, 512])\n",
      "layer_1_attention: torch.Size([1, 3, 3584])\n",
      "layer_1: torch.Size([1, 3, 3584])\n",
      "layer_2_input_layernorm: torch.Size([3, 3584])\n",
      "layer_2_q_proj: torch.Size([3, 3584])\n",
      "layer_2_k_proj: torch.Size([3, 512])\n",
      "layer_2_v_proj: torch.Size([3, 512])\n",
      "layer_2_attention: torch.Size([1, 3, 3584])\n",
      "layer_2: torch.Size([1, 3, 3584])\n",
      "layer_3_input_layernorm: torch.Size([3, 3584])\n",
      "layer_3_q_proj: torch.Size([3, 3584])\n",
      "layer_3_k_proj: torch.Size([3, 512])\n",
      "layer_3_v_proj: torch.Size([3, 512])\n",
      "layer_3_attention: torch.Size([1, 3, 3584])\n",
      "layer_3: torch.Size([1, 3, 3584])\n",
      "layer_4_input_layernorm: torch.Size([3, 3584])\n",
      "layer_4_q_proj: torch.Size([3, 3584])\n",
      "layer_4_k_proj: torch.Size([3, 512])\n",
      "layer_4_v_proj: torch.Size([3, 512])\n",
      "layer_4_attention: torch.Size([1, 3, 3584])\n",
      "layer_4: torch.Size([1, 3, 3584])\n",
      "layer_5_input_layernorm: torch.Size([3, 3584])\n",
      "layer_5_q_proj: torch.Size([3, 3584])\n",
      "layer_5_k_proj: torch.Size([3, 512])\n",
      "layer_5_v_proj: torch.Size([3, 512])\n",
      "layer_5_attention: torch.Size([1, 3, 3584])\n",
      "layer_5: torch.Size([1, 3, 3584])\n",
      "layer_6_input_layernorm: torch.Size([3, 3584])\n",
      "layer_6_q_proj: torch.Size([3, 3584])\n",
      "layer_6_k_proj: torch.Size([3, 512])\n",
      "layer_6_v_proj: torch.Size([3, 512])\n",
      "layer_6_attention: torch.Size([1, 3, 3584])\n",
      "layer_6: torch.Size([1, 3, 3584])\n",
      "layer_7_input_layernorm: torch.Size([3, 3584])\n",
      "layer_7_q_proj: torch.Size([3, 3584])\n",
      "layer_7_k_proj: torch.Size([3, 512])\n",
      "layer_7_v_proj: torch.Size([3, 512])\n",
      "layer_7_attention: torch.Size([1, 3, 3584])\n",
      "layer_7: torch.Size([1, 3, 3584])\n",
      "layer_8_input_layernorm: torch.Size([3, 3584])\n",
      "layer_8_q_proj: torch.Size([3, 3584])\n",
      "layer_8_k_proj: torch.Size([3, 512])\n",
      "layer_8_v_proj: torch.Size([3, 512])\n",
      "layer_8_attention: torch.Size([1, 3, 3584])\n",
      "layer_8: torch.Size([1, 3, 3584])\n",
      "layer_9_input_layernorm: torch.Size([3, 3584])\n",
      "layer_9_q_proj: torch.Size([3, 3584])\n",
      "layer_9_k_proj: torch.Size([3, 512])\n",
      "layer_9_v_proj: torch.Size([3, 512])\n",
      "layer_9_attention: torch.Size([1, 3, 3584])\n",
      "layer_9: torch.Size([1, 3, 3584])\n",
      "layer_10_input_layernorm: torch.Size([3, 3584])\n",
      "layer_10_q_proj: torch.Size([3, 3584])\n",
      "layer_10_k_proj: torch.Size([3, 512])\n",
      "layer_10_v_proj: torch.Size([3, 512])\n",
      "layer_10_attention: torch.Size([1, 3, 3584])\n",
      "layer_10: torch.Size([1, 3, 3584])\n",
      "layer_11_input_layernorm: torch.Size([3, 3584])\n",
      "layer_11_q_proj: torch.Size([3, 3584])\n",
      "layer_11_k_proj: torch.Size([3, 512])\n",
      "layer_11_v_proj: torch.Size([3, 512])\n",
      "layer_11_attention: torch.Size([1, 3, 3584])\n",
      "layer_11: torch.Size([1, 3, 3584])\n",
      "layer_12_input_layernorm: torch.Size([3, 3584])\n",
      "layer_12_q_proj: torch.Size([3, 3584])\n",
      "layer_12_k_proj: torch.Size([3, 512])\n",
      "layer_12_v_proj: torch.Size([3, 512])\n",
      "layer_12_attention: torch.Size([1, 3, 3584])\n",
      "layer_12: torch.Size([1, 3, 3584])\n",
      "layer_13_input_layernorm: torch.Size([3, 3584])\n",
      "layer_13_q_proj: torch.Size([3, 3584])\n",
      "layer_13_k_proj: torch.Size([3, 512])\n",
      "layer_13_v_proj: torch.Size([3, 512])\n",
      "layer_13_attention: torch.Size([1, 3, 3584])\n",
      "layer_13: torch.Size([1, 3, 3584])\n",
      "layer_14_input_layernorm: torch.Size([3, 3584])\n",
      "layer_14_q_proj: torch.Size([3, 3584])\n",
      "layer_14_k_proj: torch.Size([3, 512])\n",
      "layer_14_v_proj: torch.Size([3, 512])\n",
      "layer_14_attention: torch.Size([1, 3, 3584])\n",
      "layer_14: torch.Size([1, 3, 3584])\n",
      "layer_15_input_layernorm: torch.Size([3, 3584])\n",
      "layer_15_q_proj: torch.Size([3, 3584])\n",
      "layer_15_k_proj: torch.Size([3, 512])\n",
      "layer_15_v_proj: torch.Size([3, 512])\n",
      "layer_15_attention: torch.Size([1, 3, 3584])\n",
      "layer_15: torch.Size([1, 3, 3584])\n",
      "layer_16_input_layernorm: torch.Size([3, 3584])\n",
      "layer_16_q_proj: torch.Size([3, 3584])\n",
      "layer_16_k_proj: torch.Size([3, 512])\n",
      "layer_16_v_proj: torch.Size([3, 512])\n",
      "layer_16_attention: torch.Size([1, 3, 3584])\n",
      "layer_16: torch.Size([1, 3, 3584])\n",
      "layer_17_input_layernorm: torch.Size([3, 3584])\n",
      "layer_17_q_proj: torch.Size([3, 3584])\n",
      "layer_17_k_proj: torch.Size([3, 512])\n",
      "layer_17_v_proj: torch.Size([3, 512])\n",
      "layer_17_attention: torch.Size([1, 3, 3584])\n",
      "layer_17: torch.Size([1, 3, 3584])\n",
      "layer_18_input_layernorm: torch.Size([3, 3584])\n",
      "layer_18_q_proj: torch.Size([3, 3584])\n",
      "layer_18_k_proj: torch.Size([3, 512])\n",
      "layer_18_v_proj: torch.Size([3, 512])\n",
      "layer_18_attention: torch.Size([1, 3, 3584])\n",
      "layer_18: torch.Size([1, 3, 3584])\n",
      "layer_19_input_layernorm: torch.Size([3, 3584])\n",
      "layer_19_q_proj: torch.Size([3, 3584])\n",
      "layer_19_k_proj: torch.Size([3, 512])\n",
      "layer_19_v_proj: torch.Size([3, 512])\n",
      "layer_19_attention: torch.Size([1, 3, 3584])\n",
      "layer_19: torch.Size([1, 3, 3584])\n",
      "layer_20_input_layernorm: torch.Size([3, 3584])\n",
      "layer_20_q_proj: torch.Size([3, 3584])\n",
      "layer_20_k_proj: torch.Size([3, 512])\n",
      "layer_20_v_proj: torch.Size([3, 512])\n",
      "layer_20_attention: torch.Size([1, 3, 3584])\n",
      "layer_20: torch.Size([1, 3, 3584])\n",
      "layer_21_input_layernorm: torch.Size([3, 3584])\n",
      "layer_21_q_proj: torch.Size([3, 3584])\n",
      "layer_21_k_proj: torch.Size([3, 512])\n",
      "layer_21_v_proj: torch.Size([3, 512])\n",
      "layer_21_attention: torch.Size([1, 3, 3584])\n",
      "layer_21: torch.Size([1, 3, 3584])\n",
      "layer_22_input_layernorm: torch.Size([3, 3584])\n",
      "layer_22_q_proj: torch.Size([3, 3584])\n",
      "layer_22_k_proj: torch.Size([3, 512])\n",
      "layer_22_v_proj: torch.Size([3, 512])\n",
      "layer_22_attention: torch.Size([1, 3, 3584])\n",
      "layer_22: torch.Size([1, 3, 3584])\n",
      "layer_23_input_layernorm: torch.Size([3, 3584])\n",
      "layer_23_q_proj: torch.Size([3, 3584])\n",
      "layer_23_k_proj: torch.Size([3, 512])\n",
      "layer_23_v_proj: torch.Size([3, 512])\n",
      "layer_23_attention: torch.Size([1, 3, 3584])\n",
      "layer_23: torch.Size([1, 3, 3584])\n",
      "layer_24_input_layernorm: torch.Size([3, 3584])\n",
      "layer_24_q_proj: torch.Size([3, 3584])\n",
      "layer_24_k_proj: torch.Size([3, 512])\n",
      "layer_24_v_proj: torch.Size([3, 512])\n",
      "layer_24_attention: torch.Size([1, 3, 3584])\n",
      "layer_24: torch.Size([1, 3, 3584])\n",
      "layer_25_input_layernorm: torch.Size([3, 3584])\n",
      "layer_25_q_proj: torch.Size([3, 3584])\n",
      "layer_25_k_proj: torch.Size([3, 512])\n",
      "layer_25_v_proj: torch.Size([3, 512])\n",
      "layer_25_attention: torch.Size([1, 3, 3584])\n",
      "layer_25: torch.Size([1, 3, 3584])\n",
      "layer_26_input_layernorm: torch.Size([3, 3584])\n",
      "layer_26_q_proj: torch.Size([3, 3584])\n",
      "layer_26_k_proj: torch.Size([3, 512])\n",
      "layer_26_v_proj: torch.Size([3, 512])\n",
      "layer_26_attention: torch.Size([1, 3, 3584])\n",
      "layer_26: torch.Size([1, 3, 3584])\n",
      "layer_27_input_layernorm: torch.Size([3, 3584])\n",
      "layer_27_q_proj: torch.Size([3, 3584])\n",
      "layer_27_k_proj: torch.Size([3, 512])\n",
      "layer_27_v_proj: torch.Size([3, 512])\n",
      "layer_27_attention: torch.Size([1, 3, 3584])\n",
      "layer_27: torch.Size([1, 3, 3584])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        #print(name, output, type(output))\n",
    "        if(\"layer\" in name):\n",
    "            activations[name] = output[0].detach()  # For transformer layers, output is a tuple (last_hidden_state, past_key_values)\n",
    "        else:\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks for all transformer layers\n",
    "# DeepSeek-R1-Distill-Qwen-7B has 32 layers (layers 0-31)\n",
    "for i in range(28):\n",
    "    lm_model.model.layers[i].register_forward_hook(get_activation(f'layer_{i}'))\n",
    "    lm_model.model.layers[i].self_attn.register_forward_hook(get_activation(f'layer_{i}_attention'))\n",
    "    lm_model.model.layers[i].input_layernorm.register_forward_hook(get_activation(f'layer_{i}_input_layernorm'))\n",
    "    lm_model.model.layers[i].self_attn.q_proj.register_forward_hook(get_activation(f'layer_{i}_q_proj'))\n",
    "    lm_model.model.layers[i].self_attn.k_proj.register_forward_hook(get_activation(f'layer_{i}_k_proj'))\n",
    "    lm_model.model.layers[i].self_attn.v_proj.register_forward_hook(get_activation(f'layer_{i}_v_proj'))\n",
    "\n",
    "\"\"\"\n",
    "# You can also register hooks for specific components within layers\n",
    "# For example, attention and MLP outputs:\n",
    "for i in range(28):\n",
    "    model.layers[i].self_attn.register_forward_hook(get_activation(f'layer_{i}_attention'))\n",
    "    model.layers[i].mlp.register_forward_hook(get_activation(f'layer_{i}_mlp'))\n",
    "\"\"\"\n",
    "# Register hook for embeddings\n",
    "lm_model.model.embed_tokens.register_forward_hook(get_activation('embeddings'))\n",
    "\n",
    "# Forward pass\n",
    "outputs = lm_model(input_ids=inputs)\n",
    "\n",
    "# Access activations\n",
    "print(f\"Embeddings shape: {activations['embeddings'].shape}\")\n",
    "print(f\"Layer 0 shape: {activations['layer_0'][0].shape}\")  # Note: output is tuple, take first element\n",
    "print(f\"Layer 15 shape: {activations['layer_15'][0].shape}\")\n",
    "print(f\"Layer 27 shape: {activations['layer_27'][0].shape}\")\n",
    "\n",
    "# Print all available activation keys\n",
    "print(\"\\nAll captured activations:\")\n",
    "for key in activations.keys():\n",
    "    if isinstance(activations[key], tuple):\n",
    "        print(f\"{key}: {activations[key][0].shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {activations[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5a4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embeddings', 'layer_0_input_layernorm', 'layer_0_q_proj', 'layer_0_k_proj', 'layer_0_v_proj', 'layer_0_attention', 'layer_0', 'layer_1_input_layernorm', 'layer_1_q_proj', 'layer_1_k_proj', 'layer_1_v_proj', 'layer_1_attention', 'layer_1', 'layer_2_input_layernorm', 'layer_2_q_proj', 'layer_2_k_proj', 'layer_2_v_proj', 'layer_2_attention', 'layer_2', 'layer_3_input_layernorm', 'layer_3_q_proj', 'layer_3_k_proj', 'layer_3_v_proj', 'layer_3_attention', 'layer_3', 'layer_4_input_layernorm', 'layer_4_q_proj', 'layer_4_k_proj', 'layer_4_v_proj', 'layer_4_attention', 'layer_4', 'layer_5_input_layernorm', 'layer_5_q_proj', 'layer_5_k_proj', 'layer_5_v_proj', 'layer_5_attention', 'layer_5', 'layer_6_input_layernorm', 'layer_6_q_proj', 'layer_6_k_proj', 'layer_6_v_proj', 'layer_6_attention', 'layer_6', 'layer_7_input_layernorm', 'layer_7_q_proj', 'layer_7_k_proj', 'layer_7_v_proj', 'layer_7_attention', 'layer_7', 'layer_8_input_layernorm', 'layer_8_q_proj', 'layer_8_k_proj', 'layer_8_v_proj', 'layer_8_attention', 'layer_8', 'layer_9_input_layernorm', 'layer_9_q_proj', 'layer_9_k_proj', 'layer_9_v_proj', 'layer_9_attention', 'layer_9', 'layer_10_input_layernorm', 'layer_10_q_proj', 'layer_10_k_proj', 'layer_10_v_proj', 'layer_10_attention', 'layer_10', 'layer_11_input_layernorm', 'layer_11_q_proj', 'layer_11_k_proj', 'layer_11_v_proj', 'layer_11_attention', 'layer_11', 'layer_12_input_layernorm', 'layer_12_q_proj', 'layer_12_k_proj', 'layer_12_v_proj', 'layer_12_attention', 'layer_12', 'layer_13_input_layernorm', 'layer_13_q_proj', 'layer_13_k_proj', 'layer_13_v_proj', 'layer_13_attention', 'layer_13', 'layer_14_input_layernorm', 'layer_14_q_proj', 'layer_14_k_proj', 'layer_14_v_proj', 'layer_14_attention', 'layer_14', 'layer_15_input_layernorm', 'layer_15_q_proj', 'layer_15_k_proj', 'layer_15_v_proj', 'layer_15_attention', 'layer_15', 'layer_16_input_layernorm', 'layer_16_q_proj', 'layer_16_k_proj', 'layer_16_v_proj', 'layer_16_attention', 'layer_16', 'layer_17_input_layernorm', 'layer_17_q_proj', 'layer_17_k_proj', 'layer_17_v_proj', 'layer_17_attention', 'layer_17', 'layer_18_input_layernorm', 'layer_18_q_proj', 'layer_18_k_proj', 'layer_18_v_proj', 'layer_18_attention', 'layer_18', 'layer_19_input_layernorm', 'layer_19_q_proj', 'layer_19_k_proj', 'layer_19_v_proj', 'layer_19_attention', 'layer_19', 'layer_20_input_layernorm', 'layer_20_q_proj', 'layer_20_k_proj', 'layer_20_v_proj', 'layer_20_attention', 'layer_20', 'layer_21_input_layernorm', 'layer_21_q_proj', 'layer_21_k_proj', 'layer_21_v_proj', 'layer_21_attention', 'layer_21', 'layer_22_input_layernorm', 'layer_22_q_proj', 'layer_22_k_proj', 'layer_22_v_proj', 'layer_22_attention', 'layer_22', 'layer_23_input_layernorm', 'layer_23_q_proj', 'layer_23_k_proj', 'layer_23_v_proj', 'layer_23_attention', 'layer_23', 'layer_24_input_layernorm', 'layer_24_q_proj', 'layer_24_k_proj', 'layer_24_v_proj', 'layer_24_attention', 'layer_24', 'layer_25_input_layernorm', 'layer_25_q_proj', 'layer_25_k_proj', 'layer_25_v_proj', 'layer_25_attention', 'layer_25', 'layer_26_input_layernorm', 'layer_26_q_proj', 'layer_26_k_proj', 'layer_26_v_proj', 'layer_26_attention', 'layer_26', 'layer_27_input_layernorm', 'layer_27_q_proj', 'layer_27_k_proj', 'layer_27_v_proj', 'layer_27_attention', 'layer_27'])\n"
     ]
    }
   ],
   "source": [
    "print(activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd331ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.0006,  0.0005, -0.0049,  ..., -0.0001, -0.0004, -0.0010],\n",
       "          [-0.0060, -0.0042,  0.0084,  ...,  0.0510,  0.0008, -0.0086],\n",
       "          [-0.0225, -0.0293,  0.0107,  ..., -0.0100,  0.0104, -0.0571]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_input_layernorm': tensor([[-0.0330,  0.0282, -0.2754,  ..., -0.0080, -0.0244, -0.0520],\n",
       "         [-0.0435, -0.0325,  0.0591,  ...,  0.3574,  0.0059, -0.0566],\n",
       "         [-0.1572, -0.2197,  0.0737,  ..., -0.0688,  0.0723, -0.3652]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_q_proj': tensor([[ 1.6641,  3.5312, -0.9570,  ...,  0.3555, -0.1641,  0.5820],\n",
       "         [ 1.5234,  2.8438, -0.7656,  ..., -1.0703, -0.7930,  0.3828],\n",
       "         [ 0.6250,  2.1250, -1.3750,  ...,  0.1187, -0.3633,  0.3320]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_k_proj': tensor([[-0.4316,  1.1250,  0.5703,  ...,  2.9688, -5.7812,  8.5000],\n",
       "         [-0.9180,  1.1875,  0.5117,  ...,  3.4219, -6.4062,  8.3750],\n",
       "         [-0.4551,  1.0703,  0.4766,  ...,  2.4531, -5.0312,  9.0625]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_v_proj': tensor([[-6.9824e-02, -7.3047e-01, -8.5156e-01,  ...,  1.1406e+00,\n",
       "           1.0312e+00,  9.4238e-02],\n",
       "         [-1.2305e-01, -1.2695e-01, -1.3086e-01,  ...,  1.2779e-04,\n",
       "           4.0234e-01,  1.1279e-01],\n",
       "         [ 1.4258e-01, -1.4465e-02,  1.5820e-01,  ..., -1.7188e-01,\n",
       "          -6.3672e-01,  1.7090e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_attention': tensor([[[-0.4707,  0.8828,  0.1729,  ...,  0.5547, -0.0447, -0.2637],\n",
       "          [ 0.0693,  0.8867, -0.3281,  ...,  0.1216,  0.4512, -0.5352],\n",
       "          [ 0.0898,  0.0732,  0.4688,  ...,  0.3906,  0.7031, -0.1143]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0': tensor([[[-1.6094,  1.7422,  0.9062,  ...,  2.0938, -0.4004, -0.7383],\n",
       "          [-0.4648,  1.4219,  0.0684,  ...,  1.0000,  0.3652, -1.0312],\n",
       "          [ 0.3672,  0.4727,  0.5352,  ...,  0.9922,  0.5508,  0.1387]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_input_layernorm': tensor([[-7.5684e-02,  1.3184e-01,  3.1982e-02,  ...,  2.5868e-05,\n",
       "          -5.4016e-03, -8.2031e-02],\n",
       "         [-3.7109e-02,  1.8164e-01,  4.0588e-03,  ...,  2.0981e-05,\n",
       "           8.3618e-03, -1.9434e-01],\n",
       "         [ 4.3457e-02,  9.0332e-02,  4.7607e-02,  ...,  3.0994e-05,\n",
       "           1.8799e-02,  3.9062e-02]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_q_proj': tensor([[ 0.2031,  5.3438, -0.6875,  ...,  0.9492, -1.6016, -2.0156],\n",
       "         [ 0.3516,  5.5000, -0.6445,  ...,  0.8789, -1.7109, -2.0000],\n",
       "         [ 0.4062,  7.0938, -1.1250,  ...,  0.8633, -1.9453, -2.2656]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_k_proj': tensor([[ 0.3535, -0.7109, -0.1328,  ..., -1.9141,  2.3125,  1.7656],\n",
       "         [ 0.3164, -0.5977, -0.1523,  ..., -1.9219,  2.5156,  1.7891],\n",
       "         [-0.0198, -0.4980, -0.3711,  ..., -2.0312,  2.2031,  1.2656]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_v_proj': tensor([[-0.0967, -0.0398,  0.0422,  ..., -0.1553, -0.0781, -0.0193],\n",
       "         [-0.1357, -0.0530, -0.0215,  ..., -0.1108,  0.0258, -0.0376],\n",
       "         [-0.2637,  0.1279,  0.0108,  ...,  0.0378, -0.0430, -0.0762]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_attention': tensor([[[ 0.3848, -0.4414,  0.0162,  ..., -0.0403,  0.2139,  0.4531],\n",
       "          [ 0.3223, -0.4941,  0.0227,  ..., -0.0052,  0.1953,  0.5781],\n",
       "          [ 0.2832, -0.5273, -0.1523,  ..., -0.0280,  0.0045,  0.8555]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1': tensor([[[-1.3906,  1.4297,  0.4473,  ...,  1.8281,  0.0107, -0.3574],\n",
       "          [-0.2324,  0.8711, -0.3086,  ...,  0.6250,  0.6016, -0.5938],\n",
       "          [ 0.6523,  0.6328,  0.1484,  ...,  0.2852,  0.2637,  1.0234]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_input_layernorm': tensor([[-0.1523,  0.1904,  0.0488,  ...,  0.1143,  0.0008, -0.0649],\n",
       "         [-0.0430,  0.1953, -0.0566,  ...,  0.0654,  0.0718, -0.1816],\n",
       "         [ 0.1768,  0.2070,  0.0398,  ...,  0.0437,  0.0461,  0.4609]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_q_proj': tensor([[-0.7891, -1.1172,  2.9062,  ...,  0.0238,  0.0767, -0.1367],\n",
       "         [-0.8125, -1.3438,  3.2344,  ..., -0.0306,  0.0654, -0.3555],\n",
       "         [-1.5547, -1.2422,  3.5469,  ..., -0.0432, -0.1357,  0.2793]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_k_proj': tensor([[ -0.6484,   0.7617,  -0.2656,  ...,  -9.6875,  -5.5000,  -9.0625],\n",
       "         [ -0.7344,   1.2500,  -0.6445,  ...,  -9.8125,  -5.5000,  -9.4375],\n",
       "         [ -0.8984,   2.5625,  -1.6094,  ...,  -8.5625,  -5.2812, -10.0625]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_v_proj': tensor([[ 0.0073, -0.0032, -0.0527,  ..., -0.1406,  0.0874, -0.1079],\n",
       "         [-0.2109, -0.1309, -0.1367,  ..., -0.1855,  0.1865, -0.1943],\n",
       "         [-0.2617,  0.4219,  0.3086,  ...,  0.3926,  0.0894, -0.2188]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_attention': tensor([[[-0.2373, -0.4707, -0.4414,  ...,  0.0469, -0.0317, -0.2139],\n",
       "          [-0.2119, -0.3516, -0.4062,  ...,  0.0894, -0.2207, -0.1943],\n",
       "          [-0.0206, -0.5156, -0.4082,  ..., -0.0068, -0.3125, -0.0327]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2': tensor([[[-1.6875,  1.3750, -0.6172,  ...,  2.5625, -0.7188, -0.9922],\n",
       "          [-0.2734,  1.0000, -1.0469,  ...,  0.9883, -0.0059, -0.8672],\n",
       "          [ 0.7461,  0.4395, -0.2949,  ..., -0.0059,  0.2676,  0.6172]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_input_layernorm': tensor([[-0.1504,  0.1611, -0.0688,  ...,  0.1758, -0.0488, -0.1553],\n",
       "         [-0.0405,  0.1953, -0.1943,  ...,  0.1128, -0.0007, -0.2256],\n",
       "         [ 0.1406,  0.1089, -0.0693,  ..., -0.0008,  0.0381,  0.2031]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_q_proj': tensor([[ 0.9727, -0.1533,  0.8047,  ..., -0.3125, -0.7930,  0.1001],\n",
       "         [ 1.6797, -0.2539,  0.8555,  ..., -0.0903, -0.8438, -0.1533],\n",
       "         [ 0.8438, -0.9688,  0.5039,  ...,  0.0566, -0.8477, -0.4238]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_k_proj': tensor([[-2.6094,  1.1719,  2.2500,  ..., -8.5625, -4.4688,  0.8164],\n",
       "         [-2.7656,  1.8672,  2.6094,  ..., -8.4375, -4.7188,  0.4902],\n",
       "         [-1.9219,  1.7969,  2.8125,  ..., -9.0000, -3.3281,  0.4707]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_v_proj': tensor([[ 0.0413,  0.1001,  0.0332,  ..., -0.0967,  0.1416,  0.0830],\n",
       "         [ 0.2637,  0.2363, -0.0977,  ..., -0.2695, -0.0034,  0.0347],\n",
       "         [ 0.5508, -0.9570, -0.3926,  ...,  0.3750,  0.1914, -0.2930]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_attention': tensor([[[ 0.0623, -0.9531,  0.2832,  ...,  0.4453, -0.0933, -0.2344],\n",
       "          [-0.1533, -0.8945,  0.6211,  ...,  0.3652, -0.1260, -0.3457],\n",
       "          [-0.3438, -0.3789,  0.5469,  ...,  0.3262,  0.0820, -0.4434]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3': tensor([[[-17.8750, 137.0000,   3.0625,  ..., -68.0000,  -2.5781,  11.3125],\n",
       "          [-12.5000, 109.0000,   0.8086,  ..., -57.7500,  -6.4375,   4.5312],\n",
       "          [  1.3203,   0.2002,  -1.5859,  ...,  -0.2617,   0.7969,  -1.1484]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_input_layernorm': tensor([[-0.0129,  0.0369,  0.0025,  ..., -0.0276, -0.0008,  0.0067],\n",
       "         [-0.0111,  0.0361,  0.0008,  ..., -0.0288, -0.0026,  0.0033],\n",
       "         [ 0.3574,  0.0200, -0.4902,  ..., -0.0396,  0.0967, -0.2520]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_q_proj': tensor([[ 0.5195,  0.2256, -0.3281,  ...,  0.4531, -0.1494, -0.3926],\n",
       "         [ 0.5195,  0.2236, -0.3281,  ...,  0.4453, -0.1465, -0.3945],\n",
       "         [-0.0801,  0.1104, -0.1875,  ..., -0.5820, -0.6602,  0.0435]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_k_proj': tensor([[ 0.3379, -0.1025, -0.1128,  ..., -0.1226, -0.0356,  0.3594],\n",
       "         [ 0.3301, -0.1069, -0.1094,  ..., -0.1191, -0.0466,  0.3594],\n",
       "         [-1.2266, -1.4375,  0.9375,  ..., -1.1484,  2.2812, -0.2520]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_v_proj': tensor([[ 2.2095e-02,  5.9128e-04,  6.0120e-03,  ..., -1.9836e-03,\n",
       "           1.9531e-02,  4.7913e-03],\n",
       "         [ 2.0996e-02,  8.5068e-04,  5.8594e-03,  ..., -1.2360e-03,\n",
       "           2.3682e-02,  8.2397e-03],\n",
       "         [ 1.1797e+00, -3.5938e-01,  8.3984e-01,  ..., -3.6133e-01,\n",
       "           2.6562e-01, -6.8359e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_attention': tensor([[[ 0.0101,  0.1719,  0.0928,  ...,  0.0610,  0.0408,  0.0894],\n",
       "          [ 0.0110,  0.1709,  0.0923,  ...,  0.0623,  0.0398,  0.0889],\n",
       "          [ 0.0996,  0.1611,  0.1582,  ...,  0.2676,  0.0996, -0.1089]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4': tensor([[[-19.3750, 121.5000,  -2.6875,  ..., -71.5000,   1.7812,  12.5000],\n",
       "          [-14.1875,  93.5000,  -5.0625,  ..., -61.0000,  -2.0625,   5.6875],\n",
       "          [  0.6406,  -0.3535,  -0.9688,  ...,  -0.2500,   0.8906,  -1.9062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_input_layernorm': tensor([[-0.0167,  0.0532, -0.0028,  ..., -0.0356,  0.0008,  0.0089],\n",
       "         [-0.0146,  0.0491, -0.0063,  ..., -0.0364, -0.0011,  0.0049],\n",
       "         [ 0.2148, -0.0603, -0.3926,  ..., -0.0486,  0.1523, -0.5273]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_q_proj': tensor([[ 0.6289, -0.8398, -0.7070,  ...,  0.3926, -0.3262, -0.0874],\n",
       "         [ 0.6328, -0.8359, -0.7070,  ...,  0.3848, -0.3242, -0.0957],\n",
       "         [ 1.1641, -1.7812, -0.9258,  ...,  0.1318,  0.2432, -0.1455]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_k_proj': tensor([[-0.0220, -0.0908, -0.0825,  ...,  0.0859, -0.1885,  0.1709],\n",
       "         [-0.0292, -0.0854, -0.0864,  ...,  0.0776, -0.1865,  0.1787],\n",
       "         [-0.6719,  1.4844,  1.1562,  ..., -1.3672, -1.6406,  1.2500]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_v_proj': tensor([[ 0.0288,  0.0425, -0.0327,  ..., -0.0361, -0.0223,  0.0077],\n",
       "         [ 0.0283,  0.0388, -0.0342,  ..., -0.0364, -0.0184,  0.0097],\n",
       "         [ 0.2334,  0.3730,  0.4180,  ..., -0.2500, -0.4785, -0.3926]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_attention': tensor([[[-0.0138, -0.0167,  0.0047,  ..., -0.0486, -0.0075,  0.1040],\n",
       "          [-0.0148, -0.0178,  0.0004,  ..., -0.0505, -0.0044,  0.1040],\n",
       "          [-0.0203, -0.1328,  0.0029,  ..., -0.0659, -0.0334,  0.0098]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5': tensor([[[-2.0375e+01,  1.1850e+02, -3.1562e+00,  ..., -7.0500e+01,\n",
       "            1.1406e+00,  1.2375e+01],\n",
       "          [-1.5188e+01,  9.0500e+01, -5.5625e+00,  ..., -6.0250e+01,\n",
       "           -2.7031e+00,  5.5000e+00],\n",
       "          [ 1.3438e+00, -1.5781e+00, -1.8672e+00,  ...,  4.8828e-02,\n",
       "            2.3047e-01, -2.0000e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_input_layernorm': tensor([[-1.5259e-02,  4.5410e-02, -3.7842e-03,  ..., -3.0640e-02,\n",
       "           3.8338e-04,  7.5073e-03],\n",
       "         [-1.3550e-02,  4.1504e-02, -7.9346e-03,  ..., -3.1250e-02,\n",
       "          -1.0834e-03,  3.9673e-03],\n",
       "         [ 3.5547e-01, -2.1484e-01, -7.9297e-01,  ...,  7.5378e-03,\n",
       "           2.7344e-02, -4.2969e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_q_proj': tensor([[-0.2373, -2.8906,  3.2344,  ...,  0.1523,  1.0234, -0.1099],\n",
       "         [-0.2363, -2.8906,  3.2500,  ...,  0.1553,  1.0156, -0.1157],\n",
       "         [-1.3906, -3.3750,  3.3906,  ..., -0.2090,  0.2793,  0.1680]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_k_proj': tensor([[ 0.1260,  0.0588, -0.0194,  ...,  0.1416,  0.2002, -0.7461],\n",
       "         [ 0.1318,  0.0608, -0.0188,  ...,  0.1270,  0.1836, -0.7500],\n",
       "         [-3.2344, -0.6680,  0.7578,  ..., -0.8398, -0.9844, -2.4844]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_v_proj': tensor([[-0.0059, -0.0009, -0.0308,  ..., -0.0236,  0.0106, -0.0244],\n",
       "         [-0.0067, -0.0042, -0.0249,  ..., -0.0234,  0.0060, -0.0223],\n",
       "         [ 0.2314,  0.2578, -0.0295,  ...,  0.3594, -0.1436,  0.0483]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_attention': tensor([[[-0.0464, -0.0425,  0.1631,  ..., -0.0129,  0.0571,  0.2188],\n",
       "          [-0.0469, -0.0371,  0.1641,  ..., -0.0109,  0.0522,  0.2168],\n",
       "          [-0.2656,  0.1855,  0.4004,  ..., -0.0160, -0.2207,  0.3594]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6': tensor([[[-19.8750, 117.0000,  -1.1719,  ..., -70.5000,   2.0000,  12.7500],\n",
       "          [-14.8125,  89.0000,  -3.5625,  ..., -60.5000,  -1.8047,   5.8438],\n",
       "          [  2.3750,  -1.2422,  -1.6875,  ...,   0.4043,  -1.0078,  -1.1406]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_input_layernorm': tensor([[-0.0227,  0.0562, -0.0018,  ..., -0.0425,  0.0011,  0.0101],\n",
       "         [-0.0200,  0.0508, -0.0065,  ..., -0.0435, -0.0012,  0.0056],\n",
       "         [ 0.8477, -0.1865, -0.8086,  ...,  0.0762, -0.1709, -0.2852]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_q_proj': tensor([[-0.7812,  0.1040, -0.3945,  ..., -0.4668, -0.4668, -3.3281],\n",
       "         [-0.7812,  0.1040, -0.4023,  ..., -0.4668, -0.4590, -3.3438],\n",
       "         [ 0.3066,  0.8242, -0.7969,  ..., -0.2754,  0.3926, -3.5469]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_k_proj': tensor([[-0.0752, -0.0845,  0.0342,  ..., -2.6719,  2.1875, -8.3125],\n",
       "         [-0.0776, -0.0859,  0.0383,  ..., -2.6719,  2.1562, -8.3125],\n",
       "         [ 1.6406,  0.9102, -1.9219,  ..., -4.5312,  3.0781, -5.1875]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_v_proj': tensor([[ 5.1514e-02, -3.2227e-02, -1.5869e-02,  ..., -1.5747e-02,\n",
       "          -1.4114e-03, -1.3916e-02],\n",
       "         [ 4.8584e-02, -2.8687e-02, -1.5625e-02,  ..., -1.5442e-02,\n",
       "          -6.8359e-03, -1.5381e-02],\n",
       "         [-2.7539e-01,  2.7148e-01, -5.3906e-01,  ...,  4.0039e-01,\n",
       "          -6.2500e-01, -1.4141e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_attention': tensor([[[ 0.0786, -0.1055, -0.0601,  ..., -0.1309,  0.1924,  0.0708],\n",
       "          [ 0.0742, -0.1133, -0.0598,  ..., -0.1309,  0.1875,  0.0747],\n",
       "          [ 0.0282, -0.4180, -0.0302,  ..., -0.1465,  0.4844, -0.4668]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7': tensor([[[-2.0125e+01,  1.1650e+02, -9.6094e-01,  ..., -7.1500e+01,\n",
       "            2.2656e+00,  1.2000e+01],\n",
       "          [-1.5125e+01,  8.8500e+01, -3.3594e+00,  ..., -6.1750e+01,\n",
       "           -1.5469e+00,  5.0938e+00],\n",
       "          [ 1.8750e+00, -2.2500e+00, -2.4375e+00,  ..., -9.1406e-01,\n",
       "           -1.1133e-01, -1.2891e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_input_layernorm': tensor([[-2.4780e-02,  6.3965e-02, -1.5030e-03,  ..., -3.6621e-02,\n",
       "           1.1978e-03,  1.0193e-02],\n",
       "         [-2.2095e-02,  5.7617e-02, -6.2866e-03,  ..., -3.7598e-02,\n",
       "          -9.7275e-04,  5.1575e-03],\n",
       "         [ 5.9766e-01, -3.2227e-01, -9.9609e-01,  ..., -1.2207e-01,\n",
       "          -1.5381e-02, -2.8516e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_q_proj': tensor([[ 0.0747,  0.2637, -0.3047,  ..., -0.9688,  0.2256,  0.2910],\n",
       "         [ 0.0679,  0.2617, -0.3066,  ..., -0.9492,  0.2207,  0.2891],\n",
       "         [-0.2168,  0.7188,  0.1182,  ..., -1.5312, -0.9453,  1.4688]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_k_proj': tensor([[ 0.1235,  0.0466,  0.0315,  ...,  0.8281,  0.2217,  0.3496],\n",
       "         [ 0.1226,  0.0500,  0.0376,  ...,  0.8945,  0.2129,  0.3320],\n",
       "         [-0.7109,  0.7305,  0.0554,  ..., -1.6172, -0.9023,  1.1406]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_v_proj': tensor([[-0.0469,  0.0040,  0.0991,  ..., -0.0097, -0.0258,  0.0227],\n",
       "         [-0.0488,  0.0031,  0.0957,  ..., -0.0112, -0.0220,  0.0229],\n",
       "         [-0.3359, -0.5391,  0.4922,  ..., -0.2188,  0.9219, -0.0364]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_attention': tensor([[[-0.1406, -0.0630,  0.0356,  ..., -0.0050,  0.1514, -0.0030],\n",
       "          [-0.1465, -0.0679,  0.0386,  ..., -0.0050,  0.1504, -0.0010],\n",
       "          [-0.2207,  0.0461,  0.2119,  ...,  0.0029,  0.0425,  0.1006]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8': tensor([[[-20.8750, 117.0000,  -1.3438,  ..., -71.5000,   2.2031,  11.3750],\n",
       "          [-15.8750,  89.0000,  -3.7344,  ..., -62.0000,  -1.6250,   4.5000],\n",
       "          [  1.3281,  -2.2500,  -2.1250,  ...,  -2.0156,   1.1641,  -1.0156]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_input_layernorm': tensor([[-0.0344,  0.0781, -0.0028,  ..., -0.0339,  0.0017,  0.0125],\n",
       "         [-0.0311,  0.0708, -0.0093,  ..., -0.0349, -0.0015,  0.0059],\n",
       "         [ 0.5273, -0.3613, -1.0625,  ..., -0.2305,  0.2168, -0.2676]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_q_proj': tensor([[ 0.4336,  0.2871,  0.0723,  ...,  0.5742, -0.3574,  0.4082],\n",
       "         [ 0.4395,  0.2832,  0.0742,  ...,  0.5703, -0.3613,  0.4141],\n",
       "         [ 0.7305,  0.3516,  0.4277,  ..., -0.7734,  0.2539, -0.0564]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_k_proj': tensor([[ 0.0033,  0.0840, -0.0439,  ...,  0.6250,  0.2480,  1.7812],\n",
       "         [ 0.0131,  0.0850, -0.0386,  ...,  0.6289,  0.2393,  1.7969],\n",
       "         [ 1.0312, -0.8711,  0.2373,  ..., -1.8750,  1.5703, -2.8281]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_v_proj': tensor([[ 0.0376,  0.0549, -0.0245,  ..., -0.0126,  0.0236,  0.0374],\n",
       "         [ 0.0320,  0.0559, -0.0300,  ..., -0.0112,  0.0201,  0.0435],\n",
       "         [-0.2695, -1.4141, -1.5547,  ..., -0.7578,  0.6328,  0.1572]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_attention': tensor([[[ 0.1475, -0.1699, -0.0248,  ...,  0.0059, -0.0356, -0.0991],\n",
       "          [ 0.1436, -0.1670, -0.0250,  ..., -0.0023, -0.0342, -0.1021],\n",
       "          [-0.0684, -0.1758,  0.4121,  ..., -0.3945, -0.3613,  0.0388]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9': tensor([[[-21.8750, 116.0000,  -4.0312,  ..., -70.5000,   3.7500,   9.1250],\n",
       "          [-16.8750,  88.0000,  -6.3750,  ..., -61.0000,   0.0000,   2.1875],\n",
       "          [  0.7227,  -0.5391,   1.0078,  ...,  -1.5391,   0.1289,  -1.1250]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_input_layernorm': tensor([[-0.0315,  0.0708, -0.0086,  ..., -0.0356,  0.0029,  0.0089],\n",
       "         [-0.0291,  0.0640, -0.0161,  ..., -0.0366,  0.0000,  0.0025],\n",
       "         [ 0.2676, -0.0850,  0.5508,  ..., -0.2002,  0.0258, -0.2812]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_q_proj': tensor([[ 0.3301, -0.0708, -0.0083,  ...,  0.2363,  0.3301,  0.0381],\n",
       "         [ 0.3379, -0.0718, -0.0120,  ...,  0.2256,  0.3242,  0.0283],\n",
       "         [ 0.3379, -1.1875,  0.0304,  ...,  0.8828,  0.5508,  0.3203]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_k_proj': tensor([[ 0.0737, -0.0957, -0.0732,  ...,  0.2139, -0.5508, -0.3105],\n",
       "         [ 0.0791, -0.1035, -0.0649,  ...,  0.2246, -0.5703, -0.3086],\n",
       "         [-0.9023,  0.4395,  0.0293,  ..., -1.4141,  1.1875,  0.3965]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_v_proj': tensor([[ 0.0225, -0.0117, -0.0613,  ...,  0.0248,  0.0183,  0.0272],\n",
       "         [ 0.0153, -0.0166, -0.0618,  ...,  0.0250,  0.0219,  0.0308],\n",
       "         [-0.4102,  0.2441,  0.2061,  ...,  0.1104,  0.4219,  0.1050]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_attention': tensor([[[ 0.0688, -0.0559, -0.1504,  ..., -0.2070, -0.1396, -0.0996],\n",
       "          [ 0.0659, -0.0586, -0.1455,  ..., -0.2031, -0.1367, -0.1035],\n",
       "          [ 0.2061, -0.3281, -0.2832,  ..., -0.2305,  0.1914,  0.2637]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10': tensor([[[-21.8750, 115.5000,  -5.4375,  ..., -70.0000,   3.4219,   9.2500],\n",
       "          [-16.8750,  87.5000,  -7.7500,  ..., -60.5000,  -0.3164,   2.3125],\n",
       "          [  0.8633,  -1.2188,   0.5625,  ...,  -1.4688,   0.1406,  -0.6992]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_input_layernorm': tensor([[-3.3936e-02,  7.1289e-02, -1.2268e-02,  ..., -3.1494e-02,\n",
       "           2.6245e-03,  8.9111e-03],\n",
       "         [-3.1250e-02,  6.4453e-02, -2.0874e-02,  ..., -3.2471e-02,\n",
       "          -2.8610e-04,  2.6550e-03],\n",
       "         [ 3.2617e-01, -1.8262e-01,  3.0859e-01,  ..., -1.6016e-01,\n",
       "           2.6123e-02, -1.6406e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_q_proj': tensor([[-0.0184,  0.6523,  0.4492,  ...,  0.4023, -0.8555, -0.0312],\n",
       "         [-0.0166,  0.6602,  0.4512,  ...,  0.3965, -0.8438, -0.0298],\n",
       "         [ 0.1475,  1.4219,  0.7852,  ...,  0.8203, -1.4766, -1.0234]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_k_proj': tensor([[ 0.0120,  0.0391,  0.0186,  ..., -0.4375, -0.5625,  0.1689],\n",
       "         [ 0.0087,  0.0420,  0.0221,  ..., -0.4199, -0.5664,  0.1709],\n",
       "         [ 0.6484, -1.7969, -1.1875,  ..., -2.6094,  2.0000, -0.5078]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_v_proj': tensor([[ 0.0242,  0.1099,  0.0135,  ..., -0.0281,  0.0161,  0.0057],\n",
       "         [ 0.0237,  0.1133,  0.0098,  ..., -0.0255,  0.0189,  0.0118],\n",
       "         [ 0.6680, -0.4648,  0.5586,  ...,  0.0947, -0.1768, -0.0439]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_attention': tensor([[[ 0.0181,  0.3184, -0.2061,  ..., -0.3203, -0.1475,  0.1289],\n",
       "          [ 0.0146,  0.3242, -0.2021,  ..., -0.3145, -0.1416,  0.1299],\n",
       "          [ 0.0457,  0.1436,  0.0019,  ..., -0.2246, -0.0693,  0.1484]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11': tensor([[[-22.1250, 116.0000,  -5.5625,  ..., -69.5000,   3.4219,   9.5000],\n",
       "          [-17.1250,  88.0000,  -7.8438,  ..., -59.7500,  -0.3418,   2.5469],\n",
       "          [  0.3398,  -0.6719,   0.5352,  ...,  -1.5234,   0.3086,  -0.7188]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_input_layernorm': tensor([[-0.0422,  0.0835, -0.0140,  ..., -0.0254,  0.0035,  0.0121],\n",
       "         [-0.0386,  0.0747, -0.0234,  ..., -0.0259, -0.0004,  0.0038],\n",
       "         [ 0.1533, -0.1143,  0.3203,  ..., -0.1318,  0.0757, -0.2168]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_q_proj': tensor([[-7.0625e+00,  9.0625e-01, -1.3281e+00,  ...,  6.0938e-01,\n",
       "          -1.3672e-01,  4.6692e-03],\n",
       "         [-7.0625e+00,  9.1797e-01, -1.3281e+00,  ...,  6.1719e-01,\n",
       "          -1.4844e-01, -7.6294e-03],\n",
       "         [-7.5312e+00, -2.6758e-01, -4.8633e-01,  ...,  1.7812e+00,\n",
       "          -1.0596e-01,  6.1719e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_k_proj': tensor([[-2.0142e-03, -3.1982e-02,  3.0273e-02,  ..., -9.1797e-01,\n",
       "          -4.9316e-02,  9.1406e-01],\n",
       "         [ 9.1553e-05, -2.9785e-02,  3.2227e-02,  ..., -9.1016e-01,\n",
       "          -6.1035e-02,  9.0625e-01],\n",
       "         [-9.4531e-01,  3.8477e-01,  4.1797e-01,  ...,  1.8047e+00,\n",
       "          -8.2422e-01,  1.5469e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_v_proj': tensor([[ 0.0089, -0.0033,  0.0153,  ..., -0.0378,  0.0013,  0.0084],\n",
       "         [ 0.0156, -0.0036,  0.0217,  ..., -0.0386,  0.0066,  0.0028],\n",
       "         [-0.0096, -0.5547, -0.3047,  ...,  0.4980,  0.5312, -0.0525]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_attention': tensor([[[-0.3438,  0.0942,  0.3574,  ..., -0.4062, -0.2598,  0.0204],\n",
       "          [-0.3418,  0.0898,  0.3555,  ..., -0.4023, -0.2578,  0.0160],\n",
       "          [-0.1748, -0.0166,  0.5469,  ..., -0.6719,  0.0540,  0.1680]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12': tensor([[[-2.3000e+01,  1.1550e+02, -5.1562e+00,  ..., -6.9000e+01,\n",
       "            2.8281e+00,  9.5000e+00],\n",
       "          [-1.8125e+01,  8.7500e+01, -7.4062e+00,  ..., -5.9500e+01,\n",
       "           -9.2969e-01,  2.5938e+00],\n",
       "          [-1.0547e+00,  7.8125e-02,  2.6094e+00,  ..., -2.2656e+00,\n",
       "            5.0000e-01,  4.8828e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_input_layernorm': tensor([[-4.6387e-02,  9.7168e-02, -1.6113e-02,  ..., -2.5513e-02,\n",
       "           3.5858e-03,  1.3245e-02],\n",
       "         [-4.3213e-02,  8.6914e-02, -2.7466e-02,  ..., -2.6001e-02,\n",
       "          -1.3885e-03,  4.2725e-03],\n",
       "         [-4.5898e-01,  1.4160e-02,  1.7578e+00,  ..., -1.8066e-01,\n",
       "           1.3672e-01,  1.4648e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_q_proj': tensor([[-0.9414,  1.0625, -0.3730,  ..., -0.6523,  0.7461, -0.9180],\n",
       "         [-0.9453,  1.0469, -0.3711,  ..., -0.6602,  0.7500, -0.9219],\n",
       "         [-0.8867,  1.3125, -0.0104,  ..., -0.3555,  2.2031,  0.1836]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_k_proj': tensor([[-0.0342, -0.0659,  0.1108,  ...,  1.0312,  2.1406,  3.3906],\n",
       "         [-0.0337, -0.0591,  0.1060,  ...,  1.0391,  2.1406,  3.4062],\n",
       "         [-1.8516, -0.1348, -0.7891,  ...,  0.7891,  6.1250,  1.2109]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_v_proj': tensor([[-0.0138, -0.0162,  0.0171,  ..., -0.0131, -0.0522,  0.0162],\n",
       "         [-0.0114, -0.0124,  0.0188,  ..., -0.0048, -0.0500,  0.0066],\n",
       "         [ 0.0082, -0.5352,  0.6016,  ..., -0.2422,  1.0234,  0.5234]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_attention': tensor([[[-0.2637, -0.0708, -1.9688,  ..., -0.1553, -0.1016,  0.1582],\n",
       "          [-0.2656, -0.0630, -1.9766,  ..., -0.1572, -0.1079,  0.1631],\n",
       "          [ 0.4727, -0.1924, -2.4219,  ...,  0.3145,  0.2295, -0.1807]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13': tensor([[[-23.1250, 115.0000,  -5.6250,  ..., -68.5000,   2.7031,  10.3125],\n",
       "          [-18.2500,  87.0000,  -7.8750,  ..., -59.2500,  -1.0781,   3.3906],\n",
       "          [ -1.4844,  -0.8281,   1.8125,  ...,  -1.3672,  -0.1641,   0.8320]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_input_layernorm': tensor([[-0.0500,  0.0933, -0.0164,  ..., -0.0295,  0.0035,  0.0141],\n",
       "         [-0.0469,  0.0835, -0.0271,  ..., -0.0303, -0.0016,  0.0055],\n",
       "         [-0.6406, -0.1328,  1.0391,  ..., -0.1167, -0.0420,  0.2256]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_q_proj': tensor([[ 0.1240,  0.2754, -0.0884,  ...,  0.1709,  0.3320,  0.1245],\n",
       "         [ 0.1182,  0.2656, -0.0874,  ...,  0.1719,  0.3242,  0.1338],\n",
       "         [ 0.0986, -0.0593,  0.1992,  ...,  0.1338,  0.7656, -1.2422]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_k_proj': tensor([[-0.1089, -0.0156,  0.0176,  ...,  0.7969,  0.6367, -2.2812],\n",
       "         [-0.1050, -0.0159,  0.0258,  ...,  0.7891,  0.6484, -2.2656],\n",
       "         [-0.9219,  0.7773, -0.3672,  ...,  3.9688,  1.5703, -3.1562]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_v_proj': tensor([[ 0.0471,  0.0096, -0.0398,  ...,  0.0309,  0.0299,  0.0045],\n",
       "         [ 0.0437,  0.0059, -0.0354,  ...,  0.0325,  0.0327,  0.0032],\n",
       "         [-0.1670, -1.6094,  0.9258,  ...,  0.0532,  0.4375,  0.7617]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_attention': tensor([[[ 0.2402,  0.1270, -0.0840,  ..., -0.2139, -0.4648, -0.3984],\n",
       "          [ 0.2324,  0.1309, -0.0791,  ..., -0.2158, -0.4668, -0.4004],\n",
       "          [ 0.5430,  0.0393,  0.0276,  ..., -0.2812, -0.0304, -0.4844]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14': tensor([[[-22.6250, 114.5000,  -5.9062,  ..., -68.0000,   2.4688,  10.1250],\n",
       "          [-17.7500,  86.5000,  -8.1250,  ..., -59.2500,  -1.3047,   3.1875],\n",
       "          [ -1.8750,  -0.8516,   1.5781,  ...,  -0.7461,   0.2100,  -0.7617]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_input_layernorm': tensor([[-0.0439,  0.0742, -0.0171,  ..., -0.0238,  0.0031,  0.0131],\n",
       "         [-0.0408,  0.0664, -0.0278,  ..., -0.0245, -0.0020,  0.0049],\n",
       "         [-0.6914, -0.1050,  0.8633,  ..., -0.0496,  0.0508, -0.1875]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_q_proj': tensor([[ 0.5625, -2.2656,  0.8828,  ..., -0.5078, -0.0620,  0.1973],\n",
       "         [ 0.5547, -2.2656,  0.8711,  ..., -0.5234, -0.0496,  0.1973],\n",
       "         [ 1.9688, -3.6562, -0.0069,  ...,  0.0649,  0.4414,  0.9297]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_k_proj': tensor([[-2.3438e-02,  3.3203e-02,  5.2643e-04,  ...,  1.4844e+00,\n",
       "           2.9844e+00, -7.1484e-01],\n",
       "         [-2.6123e-02,  3.0151e-02,  1.9455e-03,  ...,  1.4844e+00,\n",
       "           2.9844e+00, -7.0703e-01],\n",
       "         [-1.5000e+00,  8.4375e-01, -8.7500e-01,  ..., -1.7500e+00,\n",
       "           3.7344e+00,  1.4746e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_v_proj': tensor([[-0.1235, -0.0522,  0.0349,  ..., -0.0131,  0.0120,  0.0659],\n",
       "         [-0.1240, -0.0576,  0.0383,  ..., -0.0172,  0.0264,  0.0669],\n",
       "         [ 0.2871, -0.9375,  1.1250,  ..., -0.3223, -0.6367, -0.4473]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_attention': tensor([[[-0.0540,  0.1084, -0.0037,  ..., -0.0364,  0.2139,  0.2930],\n",
       "          [-0.0554,  0.1050, -0.0011,  ..., -0.0286,  0.2188,  0.2852],\n",
       "          [-0.0143, -0.6797,  0.0088,  ..., -0.0488,  0.5078,  0.1104]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15': tensor([[[-22.7500, 114.0000,  -5.3438,  ..., -67.5000,   2.7344,  10.1875],\n",
       "          [-17.8750,  86.0000,  -7.5312,  ..., -58.7500,  -1.0391,   3.2344],\n",
       "          [ -1.4297,  -0.2812,   1.5000,  ...,  -1.4062,   0.6641,  -0.3262]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_input_layernorm': tensor([[-0.0522,  0.1040, -0.0179,  ..., -0.0292,  0.0042,  0.0175],\n",
       "         [-0.0486,  0.0928, -0.0298,  ..., -0.0300, -0.0019,  0.0066],\n",
       "         [-0.5625, -0.0439,  0.8594,  ..., -0.1040,  0.1768, -0.0962]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_q_proj': tensor([[ 0.3535,  0.0277,  0.3848,  ...,  0.3652,  0.3672,  0.8086],\n",
       "         [ 0.3633,  0.0291,  0.3926,  ...,  0.3672,  0.3652,  0.7930],\n",
       "         [ 1.0781, -0.4570,  1.5312,  ..., -0.2949, -0.5547, -2.2500]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_k_proj': tensor([[ 0.1113, -0.0679, -0.0420,  ..., -1.4688, -0.1504,  0.4844],\n",
       "         [ 0.1108, -0.0635, -0.0447,  ..., -1.4688, -0.1426,  0.4863],\n",
       "         [-0.5156, -0.1099, -0.3496,  ..., -0.7188, -0.0928, -0.1865]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_v_proj': tensor([[ 0.0703,  0.6797,  0.0393,  ...,  0.0261, -0.0211,  0.0189],\n",
       "         [ 0.0574,  0.6836,  0.0347,  ...,  0.0223, -0.0210,  0.0197],\n",
       "         [-0.9102,  0.5430, -0.9727,  ...,  0.0437,  0.4492, -0.1455]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_attention': tensor([[[ 0.2539, -0.2734,  0.1836,  ..., -0.2412, -0.2031,  0.0483],\n",
       "          [ 0.2578, -0.2715,  0.1865,  ..., -0.2412, -0.1973,  0.0491],\n",
       "          [ 0.1719, -0.1709, -0.0332,  ..., -0.4023, -0.3184,  0.1147]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16': tensor([[[-22.6250, 113.5000,  -4.7500,  ..., -67.0000,   2.6094,  10.0625],\n",
       "          [-17.7500,  85.5000,  -6.9375,  ..., -58.5000,  -1.1797,   3.0781],\n",
       "          [ -1.5078,   0.3359,   1.9453,  ...,  -1.4922,   0.6250,   0.1992]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_input_layernorm': tensor([[-0.0530,  0.1089, -0.0192,  ..., -0.0322,  0.0046,  0.0176],\n",
       "         [-0.0493,  0.0972, -0.0332,  ..., -0.0334, -0.0025,  0.0064],\n",
       "         [-0.5820,  0.0527,  1.2891,  ..., -0.1182,  0.1807,  0.0571]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_q_proj': tensor([[ 0.7422, -0.8047,  0.8398,  ...,  0.3867, -0.3828,  0.2363],\n",
       "         [ 0.7500, -0.7969,  0.8398,  ...,  0.3887, -0.3906,  0.2354],\n",
       "         [ 0.5859, -1.6172,  1.8672,  ...,  1.5312, -2.6406,  0.3555]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_k_proj': tensor([[-0.0742,  0.0197, -0.0325,  ..., -2.0781,  0.2559,  1.7422],\n",
       "         [-0.0693,  0.0134, -0.0342,  ..., -2.0781,  0.2383,  1.7344],\n",
       "         [-2.0781,  1.4062, -1.3438,  ..., -0.5312, -2.7188,  1.2344]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_v_proj': tensor([[-0.1260, -0.0361, -0.0150,  ..., -0.0127,  0.0084,  0.1025],\n",
       "         [-0.1230, -0.0376, -0.0156,  ..., -0.0074,  0.0135,  0.1138],\n",
       "         [ 0.0615,  0.7930, -0.5078,  ...,  0.0149,  0.9961, -0.5156]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_attention': tensor([[[ 0.3418,  0.0815, -0.1895,  ..., -0.6445,  0.2676,  0.3145],\n",
       "          [ 0.3438,  0.0933, -0.1973,  ..., -0.6406,  0.2637,  0.3105],\n",
       "          [ 0.5430,  0.0608, -0.6094,  ..., -0.5781,  0.5078,  0.4434]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17': tensor([[[-22.5000, 113.5000,  -5.1875,  ..., -67.0000,   3.0938,  10.5625],\n",
       "          [-17.6250,  85.5000,  -7.3750,  ..., -59.0000,  -0.6992,   3.5469],\n",
       "          [ -0.3008,   2.6406,   2.1875,  ...,  -1.2109,   1.2656,   1.5391]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_input_layernorm': tensor([[-0.0466,  0.1260, -0.0184,  ..., -0.0315,  0.0066,  0.0203],\n",
       "         [-0.0430,  0.1128, -0.0310,  ..., -0.0330, -0.0017,  0.0081],\n",
       "         [-0.0918,  0.4375,  1.1562,  ..., -0.0845,  0.3984,  0.4375]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_q_proj': tensor([[ 1.2266e+00, -1.2188e+00,  1.1230e-01,  ..., -5.8594e-01,\n",
       "           7.3242e-02,  1.2131e-03],\n",
       "         [ 1.2266e+00, -1.2266e+00,  1.2354e-01,  ..., -6.0547e-01,\n",
       "           7.1777e-02,  3.2349e-03],\n",
       "         [ 1.2500e+00,  2.0605e-01, -1.3574e-01,  ..., -1.8281e+00,\n",
       "           8.2031e-01, -9.6484e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_k_proj': tensor([[ 0.0339,  0.0187, -0.0259,  ...,  1.2422,  0.1338,  2.6406],\n",
       "         [ 0.0294,  0.0134, -0.0195,  ...,  1.2422,  0.1338,  2.6562],\n",
       "         [ 1.5938,  0.8945,  1.0703,  ..., -1.0234, -1.2266,  2.4375]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_v_proj': tensor([[-0.1045,  0.0649,  0.1348,  ...,  0.0171, -0.0125,  0.0942],\n",
       "         [-0.1021,  0.0654,  0.1328,  ...,  0.0254, -0.0139,  0.0884],\n",
       "         [-1.5625,  1.0547, -0.2793,  ...,  0.0383,  1.1172,  0.7852]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_attention': tensor([[[ 0.3867, -0.2773, -0.4746,  ..., -0.1377, -0.1201, -0.0718],\n",
       "          [ 0.3906, -0.2812, -0.4766,  ..., -0.1406, -0.1118, -0.0718],\n",
       "          [ 0.0830, -0.2197, -0.8320,  ..., -0.1348,  0.0571,  0.5469]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18': tensor([[[-21.6250, 113.0000,  -4.4062,  ..., -66.5000,   2.8125,  10.6875],\n",
       "          [-16.7500,  85.0000,  -6.6250,  ..., -58.7500,  -0.9844,   3.6562],\n",
       "          [ -0.3750,   3.0781,   1.0547,  ...,  -2.5000,  -0.7891,   2.0938]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_input_layernorm': tensor([[-0.0422,  0.1152, -0.0116,  ..., -0.0481,  0.0042,  0.0162],\n",
       "         [-0.0388,  0.1030, -0.0206,  ..., -0.0503, -0.0018,  0.0066],\n",
       "         [-0.0942,  0.4062,  0.3594,  ..., -0.2334, -0.1533,  0.4102]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_q_proj': tensor([[-53.2500,  -8.7500, -30.8750,  ...,  -1.7656,   0.6992,   0.3770],\n",
       "         [-53.2500,  -8.7500, -30.7500,  ...,  -1.7500,   0.7148,   0.3770],\n",
       "         [-49.0000,  -7.9688, -32.2500,  ...,  -1.7656,   1.7344,   0.5508]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_k_proj': tensor([[-1.2436e-03,  8.4839e-03,  6.7871e-02,  ...,  2.1719e+00,\n",
       "           6.0938e+00,  5.5859e-01],\n",
       "         [-2.3956e-03,  1.7014e-03,  6.4453e-02,  ...,  2.1562e+00,\n",
       "           6.0625e+00,  5.7031e-01],\n",
       "         [-1.3965e-01, -6.8750e-01, -7.7734e-01,  ...,  2.5469e+00,\n",
       "           1.1062e+01,  3.0781e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_v_proj': tensor([[ 0.0830, -0.1260,  0.0603,  ..., -0.0688,  0.1445, -0.0082],\n",
       "         [ 0.0874, -0.1133,  0.0796,  ..., -0.0713,  0.1416, -0.0073],\n",
       "         [ 0.1982, -0.3086,  0.7891,  ...,  1.4531,  0.2930, -0.8008]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_attention': tensor([[[-0.3008, -0.8125,  0.0198,  ...,  0.0981,  0.0791,  0.3418],\n",
       "          [-0.2988, -0.8086,  0.0154,  ...,  0.0933,  0.0806,  0.3438],\n",
       "          [ 0.7344, -0.8906, -0.2715,  ..., -0.2793, -0.8750,  0.3809]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19': tensor([[[-21.7500, 111.5000,  -3.0156,  ..., -66.5000,   2.4219,  10.8750],\n",
       "          [-16.8750,  83.5000,  -5.2812,  ..., -58.5000,  -1.4062,   3.9219],\n",
       "          [  2.3438,   1.9922,  -0.2188,  ...,  -4.5000,  -1.6562,   0.9922]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_input_layernorm': tensor([[-0.0381,  0.1177, -0.0074,  ..., -0.0515,  0.0037,  0.0153],\n",
       "         [-0.0349,  0.1040, -0.0153,  ..., -0.0535, -0.0025,  0.0065],\n",
       "         [ 0.4961,  0.2539, -0.0649,  ..., -0.4219, -0.3066,  0.1680]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_q_proj': tensor([[ 0.8633, -0.6562, -1.6719,  ...,  0.1934,  0.7383,  0.8203],\n",
       "         [ 0.8516, -0.6523, -1.6719,  ...,  0.2100,  0.7344,  0.8086],\n",
       "         [ 0.2637, -1.1406, -1.5781,  ...,  1.4531,  0.1245,  0.6250]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_k_proj': tensor([[-0.1235, -0.0041, -0.0417,  ..., -0.4023,  0.7070,  0.3262],\n",
       "         [-0.1250, -0.0073, -0.0339,  ..., -0.3984,  0.7109,  0.3203],\n",
       "         [-0.0474, -0.8359, -0.2061,  ..., -1.8516,  1.7188,  0.5820]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_v_proj': tensor([[ 0.0245, -0.1514, -0.1099,  ...,  0.0913, -0.0311, -0.0718],\n",
       "         [ 0.0381, -0.1426, -0.1064,  ...,  0.0923, -0.0378, -0.0806],\n",
       "         [ 0.3516,  0.1484, -0.2520,  ...,  0.3086,  1.2031,  0.2285]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_attention': tensor([[[-0.4707, -0.5430, -1.1797,  ...,  0.1377, -0.5078,  0.0292],\n",
       "          [-0.4746, -0.5469, -1.1875,  ...,  0.1328, -0.5195,  0.0322],\n",
       "          [-0.5703, -0.2578, -1.5391,  ...,  0.3711, -0.5859, -0.0674]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20': tensor([[[-22.1250, 110.0000,  -4.1250,  ..., -66.5000,   1.0078,  10.6875],\n",
       "          [-17.2500,  82.0000,  -6.3438,  ..., -58.2500,  -2.8125,   3.7656],\n",
       "          [  3.4062,   0.7773,  -1.2891,  ...,  -5.2188,  -3.2500,   2.1406]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_input_layernorm': tensor([[-0.0322,  0.1011, -0.0068,  ..., -0.0806,  0.0012,  0.0121],\n",
       "         [-0.0297,  0.0894, -0.0125,  ..., -0.0830, -0.0039,  0.0051],\n",
       "         [ 0.5469,  0.0791, -0.2363,  ..., -0.6992, -0.4199,  0.2676]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_q_proj': tensor([[-0.0747,  0.3223, -0.6328,  ..., -0.3145,  0.8516,  1.4609],\n",
       "         [-0.0737,  0.3184, -0.6328,  ..., -0.3008,  0.8398,  1.4453],\n",
       "         [-0.0532,  1.2188, -1.7812,  ...,  1.3438,  0.1416,  1.3359]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_k_proj': tensor([[ 0.0825, -0.0063, -0.0649,  ...,  0.5703, -0.2715, -0.2139],\n",
       "         [ 0.0776, -0.0108, -0.0679,  ...,  0.5547, -0.2598, -0.2236],\n",
       "         [ 0.5938, -0.2178,  0.4980,  ..., -0.3223, -2.7500, -2.3750]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_v_proj': tensor([[ 0.0085,  0.1826,  0.0791,  ..., -0.1396,  0.0317,  0.0028],\n",
       "         [ 0.0091,  0.1797,  0.0845,  ..., -0.1416,  0.0231, -0.0079],\n",
       "         [ 1.7344, -1.1250, -1.7812,  ...,  0.6172,  1.5625, -1.4375]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_attention': tensor([[[ 0.4316, -0.2598,  0.9609,  ..., -0.6406,  0.3125, -0.3457],\n",
       "          [ 0.4355, -0.2598,  0.9648,  ..., -0.6328,  0.2969, -0.3496],\n",
       "          [ 0.3828, -0.6562,  0.6016,  ..., -0.8945,  0.2100,  0.0305]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21': tensor([[[-22.7500, 108.5000,  -4.1250,  ..., -67.0000,   0.6055,   8.2500],\n",
       "          [-17.8750,  80.5000,  -6.3750,  ..., -59.0000,  -3.2969,   1.3125],\n",
       "          [  5.4375,   0.8711,  -1.7109,  ...,  -8.3750,  -4.6875,   3.5312]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_input_layernorm': tensor([[-3.1494e-02,  1.0205e-01, -5.6458e-03,  ..., -7.2754e-02,\n",
       "           6.9809e-04,  9.3384e-03],\n",
       "         [-2.9297e-02,  8.9844e-02, -1.0315e-02,  ..., -7.6172e-02,\n",
       "          -4.5166e-03,  1.7548e-03],\n",
       "         [ 8.0469e-01,  8.7402e-02, -2.4902e-01,  ..., -9.7266e-01,\n",
       "          -5.7812e-01,  4.2578e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_q_proj': tensor([[-0.0056,  1.2812, -0.5586,  ..., -0.5273, -0.0173, -0.4023],\n",
       "         [ 0.0102,  1.2891, -0.5430,  ..., -0.5391, -0.0150, -0.4180],\n",
       "         [ 1.5000,  1.1797, -0.7148,  ..., -0.8906,  2.2500,  0.9375]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_k_proj': tensor([[-1.2793e-01, -4.3335e-03, -6.2012e-02,  ..., -1.6641e+00,\n",
       "           9.4531e-01,  2.0703e-01],\n",
       "         [-1.2500e-01, -1.0986e-03, -6.5918e-02,  ..., -1.7578e+00,\n",
       "           9.2969e-01,  1.8457e-01],\n",
       "         [ 2.3438e-02, -3.0273e-01, -2.5391e-01,  ...,  2.3594e+00,\n",
       "           7.6250e+00, -4.6094e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_v_proj': tensor([[ 0.0481,  0.0938,  0.0125,  ...,  0.0214, -0.0903, -0.2158],\n",
       "         [ 0.0408,  0.0889,  0.0140,  ...,  0.0187, -0.0991, -0.2217],\n",
       "         [ 1.5469, -0.1592, -2.0625,  ...,  2.3906,  0.6836,  3.1094]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_attention': tensor([[[ 0.0903,  0.0742,  0.3770,  ..., -0.2031, -0.4219, -0.4902],\n",
       "          [ 0.0762,  0.0845,  0.3730,  ..., -0.2031, -0.4297, -0.4902],\n",
       "          [-0.2617,  0.2910,  0.5430,  ..., -0.2070, -0.1523, -0.5391]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22': tensor([[[-2.5125e+01,  1.0650e+02, -4.3125e+00,  ..., -6.7500e+01,\n",
       "           -1.5781e+00,  7.0312e+00],\n",
       "          [-2.0375e+01,  7.8500e+01, -6.5000e+00,  ..., -6.0000e+01,\n",
       "           -5.5000e+00,  5.4688e-02],\n",
       "          [ 4.0000e+00,  6.0938e+00, -1.1172e+00,  ..., -8.5625e+00,\n",
       "           -4.2500e+00,  4.4062e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_input_layernorm': tensor([[-2.8931e-02,  9.0332e-02, -4.8218e-03,  ..., -6.3477e-02,\n",
       "          -1.5335e-03,  6.9275e-03],\n",
       "         [-2.7832e-02,  7.9102e-02, -8.5449e-03,  ..., -6.6895e-02,\n",
       "          -6.3477e-03,  6.3419e-05],\n",
       "         [ 4.4141e-01,  4.9805e-01, -1.1914e-01,  ..., -7.7344e-01,\n",
       "          -3.9453e-01,  4.1602e-01]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_q_proj': tensor([[ 0.5352, -0.2832,  0.0781,  ..., -0.9492,  0.4316, -0.1289],\n",
       "         [ 0.5312, -0.2832,  0.0713,  ..., -0.9336,  0.4434, -0.1377],\n",
       "         [-0.7070, -1.1719, -0.5703,  ..., -2.1719,  0.2695,  0.5977]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_k_proj': tensor([[ 8.3008e-02, -7.8613e-02, -1.3046e-03,  ...,  5.6641e-01,\n",
       "          -2.8687e-02,  6.6016e-01],\n",
       "         [ 7.6172e-02, -6.9336e-02,  1.6098e-03,  ...,  5.6641e-01,\n",
       "          -3.6865e-02,  6.4844e-01],\n",
       "         [ 5.4297e-01,  6.9531e-01, -1.4941e-01,  ...,  1.6875e+00,\n",
       "           1.3281e+00,  1.7266e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_v_proj': tensor([[-0.0752,  0.1060, -0.0164,  ...,  0.2578,  0.0488, -0.1045],\n",
       "         [-0.0757,  0.1025, -0.0217,  ...,  0.2520,  0.0464, -0.1123],\n",
       "         [ 0.5781,  2.8281,  0.9492,  ...,  0.2432,  0.6758,  0.8281]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_attention': tensor([[[-0.0233,  0.6523,  0.2676,  ...,  0.6523, -0.7383,  0.4941],\n",
       "          [-0.0214,  0.6602,  0.2637,  ...,  0.6484, -0.7383,  0.4980],\n",
       "          [-1.4688,  1.3047, -1.7109,  ...,  1.8672, -0.0442,  1.6016]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23': tensor([[[-27.3750, 105.0000,  -3.9375,  ..., -68.0000,  -5.6875,   6.1875],\n",
       "          [-22.7500,  77.0000,  -6.2812,  ..., -60.0000,  -9.6250,  -0.8516],\n",
       "          [ -0.9688,   8.7500,  -4.6562,  ..., -11.5000,  -1.7344,   3.3750]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_input_layernorm': tensor([[-0.0332,  0.1060, -0.0050,  ..., -0.0723, -0.0064,  0.0071],\n",
       "         [-0.0330,  0.0923, -0.0095,  ..., -0.0752, -0.0129, -0.0012],\n",
       "         [-0.0854,  0.6367, -0.4297,  ..., -0.8789, -0.1416,  0.2793]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_q_proj': tensor([[-2.7969,  0.4941,  0.7422,  ...,  1.5703,  0.9180,  0.9023],\n",
       "         [-2.7969,  0.4922,  0.7305,  ...,  1.5625,  0.9219,  0.8945],\n",
       "         [ 0.6758, -0.5312,  0.4414,  ...,  0.7070,  0.2021, -0.4043]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_k_proj': tensor([[-0.0552, -0.0142,  0.1064,  ..., -1.4844, -1.0703, -0.5898],\n",
       "         [-0.0640, -0.0225,  0.1050,  ..., -1.5703, -1.0547, -0.5625],\n",
       "         [ 2.3750,  0.5664, -0.6914,  ...,  2.7031, -7.0000, -5.6250]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_v_proj': tensor([[ 1.6602e-01, -4.4678e-02, -1.5820e-01,  ..., -5.0537e-02,\n",
       "           2.1118e-02,  2.5977e-01],\n",
       "         [ 1.6699e-01, -5.5908e-02, -1.5039e-01,  ..., -5.2734e-02,\n",
       "           5.3406e-03,  2.6172e-01],\n",
       "         [ 3.3789e-01, -3.9648e-01,  1.2354e-01,  ...,  4.5312e+00,\n",
       "          -5.4688e+00,  3.9531e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_attention': tensor([[[ 0.8516,  0.5977,  1.2891,  ...,  0.7500,  0.4531, -0.8750],\n",
       "          [ 0.8555,  0.5898,  1.2734,  ...,  0.7422,  0.4512, -0.8633],\n",
       "          [-0.0776,  1.8516,  0.5703,  ...,  0.3789,  0.9766, -1.0469]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24': tensor([[[-28.8750, 100.0000,  -1.5156,  ..., -67.0000,  -9.8125,   1.8438],\n",
       "          [-24.2500,  72.0000,  -3.9531,  ..., -59.0000, -13.7500,  -5.0938],\n",
       "          [ -1.9531,  11.1875,  -7.4375,  ..., -13.3750,   1.2266,   2.7969]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_input_layernorm': tensor([[-0.0354,  0.1011, -0.0019,  ..., -0.0723, -0.0112,  0.0020],\n",
       "         [-0.0352,  0.0864, -0.0058,  ..., -0.0757, -0.0186, -0.0065],\n",
       "         [-0.1377,  0.6523, -0.5312,  ..., -0.8320,  0.0806,  0.1738]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_q_proj': tensor([[ 0.6680,  0.9062,  0.5195,  ..., -0.2852, -6.0625, -2.6562],\n",
       "         [ 0.6641,  0.8984,  0.5352,  ..., -0.2676, -6.0312, -2.6562],\n",
       "         [ 0.0505,  0.0801, -0.9102,  ..., -0.7031, -4.0938, -0.6875]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_k_proj': tensor([[-0.0571,  0.0615,  0.0801,  ..., -0.7188, -0.3691,  1.0312],\n",
       "         [-0.0422,  0.0498,  0.0806,  ..., -0.7305, -0.3867,  1.0312],\n",
       "         [ 2.0781,  0.8867, -0.6523,  ...,  0.6484, -0.3574,  0.3867]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_v_proj': tensor([[ 0.2412,  0.1875, -0.0952,  ..., -0.2158, -0.1484, -0.1074],\n",
       "         [ 0.2461,  0.1787, -0.0942,  ..., -0.2168, -0.1514, -0.0942],\n",
       "         [ 0.9609,  1.5156, -0.1748,  ...,  3.4062, -6.1250, -1.3516]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_attention': tensor([[[ 0.2559, -0.6523, -0.1299,  ...,  0.7305, -0.9453, -0.1396],\n",
       "          [ 0.2598, -0.6211, -0.1543,  ...,  0.7500, -0.9609, -0.1309],\n",
       "          [ 0.2031, -0.5078, -0.6055,  ...,  1.3359, -1.0703, -0.1934]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25': tensor([[[-45.7500,  84.5000,  10.5625,  ..., -83.0000, -53.5000, -11.0625],\n",
       "          [-39.0000,  57.5000,   7.4062,  ..., -73.0000, -53.7500, -17.2500],\n",
       "          [ -3.3594,  12.1875,  -8.0625,  ..., -10.0000,   0.9961,   5.7500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_input_layernorm': tensor([[-0.0515,  0.0693,  0.0099,  ..., -0.0820, -0.0513, -0.0110],\n",
       "         [-0.0520,  0.0562,  0.0083,  ..., -0.0850, -0.0613, -0.0205],\n",
       "         [-0.1768,  0.4707, -0.3574,  ..., -0.4609,  0.0449,  0.2695]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_q_proj': tensor([[-0.0233, -0.6094,  0.3145,  ..., -0.9297, -0.2871, -0.3906],\n",
       "         [-0.0116, -0.5977,  0.3125,  ..., -0.8984, -0.2852, -0.3828],\n",
       "         [ 0.1406, -0.9570,  1.0859,  ...,  2.0469, -1.3672, -1.4922]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_k_proj': tensor([[-0.0292,  0.1914, -0.1177,  ...,  0.6445,  0.4570, -0.0923],\n",
       "         [-0.0264,  0.1895, -0.1245,  ...,  0.6680,  0.4707, -0.0986],\n",
       "         [ 0.0496, -0.1611, -0.2812,  ...,  3.2812, -2.1719, -1.1641]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_v_proj': tensor([[-0.2500, -0.2188,  0.2471,  ..., -0.3301,  0.2041,  0.0894],\n",
       "         [-0.2129, -0.2393,  0.1963,  ..., -0.3379,  0.2178,  0.0562],\n",
       "         [-4.2500, -5.5000, -3.8750,  ...,  1.7969, -0.4375, -0.6602]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_attention': tensor([[[-1.2578e+00, -2.3145e-01, -5.3906e-01,  ..., -1.4531e+00,\n",
       "            6.6406e-01, -3.8719e-04],\n",
       "          [-1.2422e+00, -2.6172e-01, -5.5078e-01,  ..., -1.4766e+00,\n",
       "            6.5625e-01, -7.2937e-03],\n",
       "          [-1.1484e+00, -1.3359e+00, -4.2773e-01,  ..., -1.2266e+00,\n",
       "            2.4805e-01, -5.1953e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26': tensor([[[ 2.1000e+01,  1.1500e+02, -3.0000e+01,  ..., -1.2700e+02,\n",
       "           -2.0750e+01,  2.7500e+00],\n",
       "          [ 2.7250e+01,  9.2000e+01, -3.0375e+01,  ..., -1.1700e+02,\n",
       "           -1.9500e+01, -6.2500e-01],\n",
       "          [ 9.3750e-02,  9.7500e+00, -7.9375e+00,  ..., -8.9375e+00,\n",
       "            1.1953e+00,  7.3750e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_input_layernorm': tensor([[ 0.0264,  0.1250, -0.0369,  ..., -0.1602, -0.0255,  0.0031],\n",
       "         [ 0.0442,  0.1289, -0.0481,  ..., -0.1895, -0.0310, -0.0009],\n",
       "         [ 0.0028,  0.2480, -0.2295,  ..., -0.2637,  0.0347,  0.1934]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_q_proj': tensor([[-0.0625, -1.3203,  0.7773,  ...,  0.9648,  0.8242,  1.9766],\n",
       "         [-0.1787, -1.3047,  0.8242,  ...,  0.9727,  0.8164,  1.9219],\n",
       "         [-0.1309, -1.3516,  0.4102,  ...,  1.8203, -3.4219, -0.1416]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_k_proj': tensor([[-0.1050, -0.6758,  0.3262,  ...,  0.0117, -2.0312, -2.3438],\n",
       "         [-0.0835, -0.7070,  0.3223,  ...,  0.0408, -1.9922, -2.3438],\n",
       "         [-0.0991,  0.2656, -0.1309,  ...,  2.5156,  0.0503, -0.2656]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_v_proj': tensor([[  0.0283,  -0.2354,  -0.8555,  ...,   0.1338,  -0.5625,   0.3828],\n",
       "         [  0.0167,  -0.5078,  -0.8906,  ...,   0.0354,  -0.7539,   0.4199],\n",
       "         [ -1.3594, -13.5000,  -1.5156,  ...,   0.4180,  -5.2188, -15.5625]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_attention': tensor([[[-47.0000, -25.6250,  16.7500,  ...,  17.3750, -41.2500, -28.7500],\n",
       "          [-46.7500, -25.2500,  16.6250,  ...,  17.3750, -41.0000, -28.5000],\n",
       "          [-16.1250, -15.0625,   6.9062,  ...,   1.5469, -13.6875, -14.7500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27': tensor([[[ 23.5000, 113.0000, -21.5000,  ..., -65.0000,  54.0000,  -5.2500],\n",
       "          [ 27.0000,  89.0000, -19.2500,  ..., -51.7500,  58.5000,  -6.1250],\n",
       "          [-22.7500,  -1.8750,  -5.5625,  ...,  -8.9375,  -7.3438,   1.8750]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16bbfdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-0.2520,  1.5391,  0.8359,  ..., -2.0625, -2.0625, -2.0625],\n",
       "         [ 0.4375,  2.5938,  1.3359,  ..., -2.4062, -2.4062, -2.4062],\n",
       "         [25.3750,  9.1875,  3.6094,  ...,  2.3438,  2.3438,  2.3438]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x76fd0cd03d00>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf63672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted next token ID: 0\n",
      "Predicted next token: '!'\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "last_token_logits = logits[0, -1, :] \n",
    "predicted_token_id = torch.argmax(last_token_logits, dim=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(f\"\\nPredicted next token ID: {predicted_token_id}\")\n",
    "print(f\"Predicted next token: '{predicted_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7597fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_hf = activations['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85805087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[[-0.0006,  0.0005, -0.0049,  ..., -0.0001, -0.0004, -0.0010],\n",
      "         [-0.0060, -0.0042,  0.0084,  ...,  0.0510,  0.0008, -0.0086],\n",
      "         [ 0.0299,  0.0116,  0.0133,  ..., -0.0177, -0.0156, -0.0669]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16) torch.Size([1, 3, 3584]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings:\", activations['embeddings'], activations['embeddings'].shape, activations['embeddings'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4899ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54579ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: tensor([[[-1.6078,  1.7398,  0.9056,  ...,  2.0966, -0.3973, -0.7354],\n",
      "         [-0.4516,  1.4271,  0.0635,  ...,  0.9967,  0.3587, -1.0365],\n",
      "         [ 0.3759,  0.4770,  0.5341,  ...,  0.9871,  0.5478,  0.1407]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 0:\", activations['layer_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41caa2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: tensor([[[-0.4713,  0.8825,  0.1737,  ...,  0.5539, -0.0437, -0.2646],\n",
      "         [ 0.0797,  0.8864, -0.3313,  ...,  0.1187,  0.4419, -0.5363],\n",
      "         [ 0.0925,  0.0708,  0.4678,  ...,  0.3931,  0.7030, -0.1090]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 0:\", activations['layer_0_attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1d84ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: tensor([[-0.0330,  0.0282, -0.2753,  ..., -0.0080, -0.0245, -0.0518],\n",
      "        [-0.0433, -0.0326,  0.0590,  ...,  0.3580,  0.0059, -0.0567],\n",
      "        [-0.1573, -0.2197,  0.0736,  ..., -0.0686,  0.0723, -0.3675]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 0:\", activations['layer_0_input_layernorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631761da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "import torch._inductor.config\n",
    "from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523f091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "from tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e466fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(\"checkpoints/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/model.pth\")\n",
    "tokenizer_path = checkpoint_path.parent / \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6ae734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87457817",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m precision \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     35\u001b[0m use_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39msetup_caches(max_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m212\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(checkpoint_path, device, precision, use_tp)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying tensor parallel to model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     apply_tp(model)\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:1003\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/fast/lib/python3.9/site-packages/torch/nn/modules/module.py:1348\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "def _load_model(checkpoint_path, device, precision, use_tp):\n",
    "    use_cuda = 'cuda' in device\n",
    "    model = Transformer.from_name(checkpoint_path.parent.name).to_empty(device=device, dtype=precision)\n",
    "\n",
    "    if \"int8\" in str(checkpoint_path):\n",
    "        print(\"Using int8 weight-only quantization!\")\n",
    "        from quantize import WeightOnlyInt8QuantHandler\n",
    "        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n",
    "        model = simple_quantizer.convert_for_runtime()\n",
    "\n",
    "    if \"int4\" in str(checkpoint_path):\n",
    "        print(\"Using int4 weight-only quantization!\")\n",
    "        path_comps = checkpoint_path.name.split(\".\")\n",
    "        groupsize = int(path_comps[-2][1:])\n",
    "        from quantize import WeightOnlyInt4QuantHandler\n",
    "        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n",
    "        model = simple_quantizer.convert_for_runtime()\n",
    "\n",
    "    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n",
    "    if \"model\" in checkpoint and \"stories\" in str(checkpoint_path):\n",
    "        checkpoint = checkpoint[\"model\"]\n",
    "    model.load_state_dict(checkpoint, assign=True)\n",
    "\n",
    "    if use_tp:\n",
    "        from tp import apply_tp\n",
    "        print(\"Applying tensor parallel to model ...\")\n",
    "        apply_tp(model)\n",
    "\n",
    "    model = model.to(device=device, dtype=precision)\n",
    "    return model.eval()\n",
    "\n",
    "device = \"cuda\"\n",
    "precision = torch.bfloat16\n",
    "use_tp = False\n",
    "\n",
    "model = _load_model(checkpoint_path, device, precision, use_tp)\n",
    "with torch.device(device):\n",
    "    model.setup_caches(max_batch_size=1, max_seq_length=212)\n",
    "model.causal_mask.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d64e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1797, 0.1924, 0.1768,  ..., 0.1758, 0.1777, 0.1650], device='cuda:0',\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].attention_norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e76ee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6152e-03, -2.1606e-02, -8.9111e-03,  ..., -6.9580e-03,\n",
       "          3.6621e-02, -1.3245e-02],\n",
       "        [ 4.4922e-02,  4.7363e-02,  1.6602e-02,  ...,  7.2021e-03,\n",
       "          2.9564e-04, -1.6235e-02],\n",
       "        [-1.6724e-02, -6.4392e-03,  1.4400e-04,  ...,  1.0824e-04,\n",
       "         -2.1973e-02,  2.3804e-03],\n",
       "        ...,\n",
       "        [-1.1755e-37,  1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37,  1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37, -1.1755e-37,  ...,  1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tok_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6e34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokenizer, string, bos=True, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(string)\n",
    "    if bos:\n",
    "        tokens = [tokenizer.bos_id()] + tokens\n",
    "    return torch.tensor(tokens, dtype=torch.int, device=device)\n",
    "encoded = encode_tokens(tokenizer, \"Hello World\", bos=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a3ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = encoded.view(1, -1).repeat(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229171d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151646,   9707,   4337]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5462d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646,   9707,   1879]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[0][2] = 1879\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0bdf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = prompt.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b021e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pos = torch.arange(0, prompt_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b04520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57aefcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlm_model\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(lm_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight, model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(lm_model.model.layers[0].self_attn.q_proj.weight)\n",
    "print(model.layers[0].attention.q_proj.weight)\n",
    "print(torch.allclose(lm_model.model.layers[0].self_attn.q_proj.weight, model.layers[0].attention.q_proj.weight, rtol=1e-5, atol=1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fe16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([3584, 3584]) torch.Size([3584, 3584])\n",
      "Same shape: True\n",
      "Allclose: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the two tensors\n",
    "a = lm_model.model.layers[0].self_attn.q_proj.weight\n",
    "b = model.layers[0].attention.q_proj.weight\n",
    "\n",
    "# Check if they are close numerically\n",
    "print(\"Shapes:\", a.shape, b.shape)\n",
    "print(\"Same shape:\", a.shape == b.shape)\n",
    "\n",
    "# Check elementwise closeness\n",
    "equal = torch.allclose(a, b, rtol=1e-5, atol=1e-8)\n",
    "print(\"Allclose:\", equal)\n",
    "\n",
    "# If not equal, show diagnostic\n",
    "if not equal:\n",
    "    abs_diff = (a - b).abs()\n",
    "    print(\"Max abs diff:\", abs_diff.max().item())\n",
    "    print(\"Mean abs diff:\", abs_diff.mean().item())\n",
    "    print(\"Std abs diff:\", abs_diff.std().item())\n",
    "\n",
    "    # Optionally, see where they differ most\n",
    "    idx = torch.argmax(abs_diff)\n",
    "    i, j = divmod(idx.item(), a.shape[1])\n",
    "    print(f\"Largest diff at position ({i}, {j}): {a[i,j].item()} vs {b[i,j].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b1b49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations[name] = output[0].detach()\n",
    "        else:\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.tok_embeddings.register_forward_hook(get_activation('embeddings'))\n",
    "for i, block in enumerate(model.layers):\n",
    "    block.register_forward_hook(get_activation(f'layer_{i}_output'))\n",
    "    block.attention.register_forward_hook(get_activation(f'layer_{i}_attention'))\n",
    "    block.attention_norm.register_forward_hook(get_activation(f'layer_{i}_attn_norm'))\n",
    "    block.attention.q_proj.register_forward_hook(get_activation(f'layer_{i}_q_proj'))\n",
    "    block.attention.k_proj.register_forward_hook(get_activation(f'layer_{i}_k_proj'))\n",
    "    block.attention.v_proj.register_forward_hook(get_activation(f'layer_{i}_v_proj'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c50c984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes torch.Size([1, 3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n",
    "    q = torch.empty_like(probs_sort).exponential_(1)\n",
    "    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "\n",
    "def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
    "    logits = logits / max(temperature, 1e-5)\n",
    "\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
    "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
    "    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n",
    "    idx_next = multinomial_sample_one_no_sync(probs)\n",
    "    return idx_next, probs\n",
    "def prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n",
    "    # input_pos: [B, S]\n",
    "    logits = model(x, input_pos)\n",
    "    return sample(logits, **sampling_kwargs)[0]\n",
    "print(\"shapes\", prompt.shape, input_pos.shape)\n",
    "next_token = prefill(model, prompt, input_pos, temperature=0, top_k=1).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e9883c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.0006,  0.0005, -0.0049,  ..., -0.0001, -0.0004, -0.0010],\n",
       "          [-0.0060, -0.0042,  0.0084,  ...,  0.0510,  0.0008, -0.0086],\n",
       "          [-0.0225, -0.0293,  0.0107,  ..., -0.0100,  0.0104, -0.0571]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_attn_norm': tensor([[[-0.0330,  0.0282, -0.2754,  ..., -0.0080, -0.0244, -0.0520],\n",
       "          [-0.0435, -0.0325,  0.0591,  ...,  0.3574,  0.0059, -0.0566],\n",
       "          [-0.1572, -0.2197,  0.0737,  ..., -0.0688,  0.0723, -0.3652]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_q_proj': tensor([[[ 1.6641,  1.5234,  0.2852,  ...,  0.1426, -0.2324,  0.5820],\n",
       "          [ 1.5234,  1.5938, -0.3984,  ..., -0.4883, -0.3789,  0.3828],\n",
       "          [ 0.6250,  2.2031, -1.1250,  ..., -0.0559,  0.4629,  0.3320]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_k_proj': tensor([[[-0.4316,  1.1406,  0.4277,  ...,  3.0156, -5.6250,  8.5000],\n",
       "          [-0.9180,  1.0234,  0.4961,  ...,  2.3906, -5.2188,  8.3750],\n",
       "          [-0.4551,  1.2344,  0.3711,  ...,  3.7656, -7.5938,  9.0625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_v_proj': tensor([[[-6.9824e-02, -7.3047e-01, -8.5156e-01,  ...,  1.1406e+00,\n",
       "            1.0312e+00,  9.4238e-02],\n",
       "          [-1.2305e-01, -1.2695e-01, -1.3086e-01,  ...,  1.2779e-04,\n",
       "            4.0234e-01,  1.1279e-01],\n",
       "          [ 1.4258e-01, -1.4465e-02,  1.5820e-01,  ..., -1.7188e-01,\n",
       "           -6.3672e-01,  1.7090e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_attention': tensor([[[-0.4707,  0.8828,  0.1729,  ...,  0.5547, -0.0447, -0.2637],\n",
       "          [-0.2168,  0.8906,  0.1001,  ...,  0.4336,  0.4316, -0.6133],\n",
       "          [-0.0344,  0.2520,  0.1904,  ...,  0.2021,  0.2656, -0.5156]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_output': tensor([[[-1.6094,  1.7422,  0.9062,  ...,  2.0938, -0.4004, -0.7383],\n",
       "          [-1.1016,  1.3750,  0.7422,  ...,  1.5938,  0.2578, -0.9688],\n",
       "          [-0.4883,  0.3535,  0.4258,  ...,  1.2969, -0.0840, -0.4727]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_attn_norm': tensor([[[-7.5684e-02,  1.3184e-01,  3.1982e-02,  ...,  2.5868e-05,\n",
       "           -5.4016e-03, -8.2031e-02],\n",
       "          [-6.9824e-02,  1.4062e-01,  3.5156e-02,  ...,  2.6584e-05,\n",
       "            4.6997e-03, -1.4551e-01],\n",
       "          [-4.4678e-02,  5.2002e-02,  2.9053e-02,  ...,  3.0994e-05,\n",
       "           -2.1973e-03, -1.0205e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_q_proj': tensor([[[ 0.2031,  0.2617,  3.1250,  ...,  0.3711, -1.2266, -2.0156],\n",
       "          [ 0.3770,  0.4570,  2.8281,  ...,  0.3066, -1.2812, -1.9531],\n",
       "          [ 0.3320,  0.1138,  3.0938,  ...,  0.1514, -1.2344, -2.1250]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_k_proj': tensor([[[ 0.3535,  0.4902, -1.0938,  ..., -3.0781,  0.1865,  1.7656],\n",
       "          [ 0.3516,  0.4082, -1.0156,  ..., -2.9219,  0.2910,  1.7188],\n",
       "          [ 0.3066,  0.4395, -0.9062,  ..., -3.0312,  0.1641,  1.4453]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_v_proj': tensor([[[-0.0967, -0.0398,  0.0422,  ..., -0.1553, -0.0781, -0.0193],\n",
       "          [-0.0938, -0.0630,  0.0312,  ..., -0.1484, -0.0476, -0.0161],\n",
       "          [-0.1494, -0.0204,  0.0640,  ..., -0.0728, -0.0874,  0.0079]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_attention': tensor([[[ 0.3848, -0.4414,  0.0162,  ..., -0.0403,  0.2139,  0.4531],\n",
       "          [ 0.3125, -0.5234,  0.0408,  ...,  0.0029,  0.2500,  0.5078],\n",
       "          [ 0.3926, -0.6406, -0.0128,  ..., -0.0908,  0.1396,  0.4629]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_output': tensor([[[-1.3906,  1.4297,  0.4473,  ...,  1.8281,  0.0107, -0.3574],\n",
       "          [-0.9453,  0.8672,  0.3535,  ...,  1.4141,  0.6680, -0.5312],\n",
       "          [-0.1602,  0.1621,  0.1172,  ...,  0.8984,  0.2061,  0.0055]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_attn_norm': tensor([[[-0.1523,  0.1904,  0.0488,  ...,  0.1143,  0.0008, -0.0649],\n",
       "          [-0.1396,  0.1553,  0.0518,  ...,  0.1182,  0.0640, -0.1299],\n",
       "          [-0.0339,  0.0415,  0.0245,  ...,  0.1079,  0.0281,  0.0019]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_q_proj': tensor([[[-0.7891, -0.6992,  2.1250,  ..., -0.1318,  0.7109, -0.1367],\n",
       "          [-0.7070, -0.5820,  2.0156,  ..., -0.1758,  0.5430, -0.2207],\n",
       "          [-0.8164, -0.6836,  2.0625,  ..., -0.2109,  0.6289, -0.1289]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_k_proj': tensor([[[-0.6484,  2.7031, -1.6797,  ..., -9.9375, -5.5000, -9.0625],\n",
       "          [-0.6445,  2.4219, -1.4844,  ..., -9.9375, -5.5000, -9.2500],\n",
       "          [-0.5625,  2.0000, -1.2656,  ..., -9.6250, -6.2188, -8.8125]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_v_proj': tensor([[[ 0.0073, -0.0032, -0.0527,  ..., -0.1406,  0.0874, -0.1079],\n",
       "          [-0.1348, -0.0928, -0.0669,  ..., -0.2197,  0.1182, -0.1621],\n",
       "          [-0.1279, -0.0649,  0.1079,  ..., -0.0493,  0.1069, -0.2148]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_attention': tensor([[[-0.2373, -0.4707, -0.4414,  ...,  0.0469, -0.0317, -0.2139],\n",
       "          [-0.2168, -0.4395, -0.4355,  ...,  0.0801, -0.1357, -0.2324],\n",
       "          [-0.1631, -0.5312, -0.3008,  ...,  0.1553, -0.0723, -0.2236]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_output': tensor([[[-1.6875,  1.3750, -0.6172,  ...,  2.5625, -0.7188, -0.9922],\n",
       "          [-1.1250,  0.8672, -0.5938,  ...,  1.7812, -0.0078, -0.9688],\n",
       "          [-0.2656,  0.0059, -0.6133,  ...,  1.2891, -0.1025, -0.2891]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_attn_norm': tensor([[[-0.1504,  0.1611, -0.0688,  ...,  0.1758, -0.0488, -0.1553],\n",
       "          [-0.1367,  0.1387, -0.0908,  ...,  0.1670, -0.0007, -0.2080],\n",
       "          [-0.0464,  0.0014, -0.1338,  ...,  0.1729, -0.0136, -0.0889]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_q_proj': tensor([[[ 0.9727, -0.2910,  0.4258,  ..., -0.4883, -0.8164,  0.1001],\n",
       "          [ 1.3125, -0.4668,  0.3770,  ..., -0.5078, -0.6523,  0.0586],\n",
       "          [ 1.2578, -0.4102,  0.2051,  ..., -0.5547, -0.8750, -0.1787]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_k_proj': tensor([[[-2.6094,  0.7695,  3.5000,  ..., -9.1250, -3.7188,  0.8164],\n",
       "          [-2.7812,  0.4160,  3.9688,  ..., -9.3125, -3.6719,  0.6211],\n",
       "          [-2.9062, -0.2314,  4.4375,  ..., -9.1250, -3.8438,  0.2148]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_v_proj': tensor([[[ 0.0413,  0.1001,  0.0332,  ..., -0.0967,  0.1416,  0.0830],\n",
       "          [ 0.0972,  0.1699, -0.0493,  ..., -0.1445,  0.0332,  0.0688],\n",
       "          [ 0.1758, -0.4863, -0.1289,  ...,  0.1406,  0.0618,  0.0077]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_attention': tensor([[[ 0.0623, -0.9531,  0.2832,  ...,  0.4453, -0.0933, -0.2344],\n",
       "          [-0.1377, -0.8672,  0.4961,  ...,  0.3613, -0.0178, -0.3535],\n",
       "          [-0.0898, -1.2266,  0.4609,  ..., -0.0024,  0.1196, -0.7109]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_output': tensor([[[-17.8750, 137.0000,   3.0625,  ..., -68.0000,  -2.5781,  11.3125],\n",
       "          [-16.7500, 137.0000,   2.2344,  ..., -71.0000,  -4.6250,   8.1250],\n",
       "          [-11.7500, 102.5000,   1.4688,  ..., -57.5000,  -3.8281,   5.8438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_attn_norm': tensor([[[-0.0129,  0.0369,  0.0025,  ..., -0.0276, -0.0008,  0.0067],\n",
       "          [-0.0120,  0.0364,  0.0018,  ..., -0.0287, -0.0015,  0.0047],\n",
       "          [-0.0110,  0.0356,  0.0016,  ..., -0.0302, -0.0016,  0.0044]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_q_proj': tensor([[[ 0.5195, -0.0781, -0.2412,  ...,  0.4492, -0.3496, -0.3926],\n",
       "          [ 0.5195, -0.0811, -0.2412,  ...,  0.4512, -0.3496, -0.3945],\n",
       "          [ 0.5195, -0.0771, -0.2432,  ...,  0.4551, -0.3496, -0.3984]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_k_proj': tensor([[[ 0.3379,  0.5547,  0.2344,  ...,  0.1206, -0.1562,  0.3594],\n",
       "          [ 0.3379,  0.5547,  0.2314,  ...,  0.1143, -0.1582,  0.3594],\n",
       "          [ 0.3340,  0.5547,  0.2324,  ...,  0.1069, -0.1631,  0.3574]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_v_proj': tensor([[[ 0.0221,  0.0006,  0.0060,  ..., -0.0020,  0.0195,  0.0048],\n",
       "          [ 0.0216,  0.0005,  0.0055,  ..., -0.0021,  0.0214,  0.0059],\n",
       "          [ 0.0225,  0.0031,  0.0075,  ..., -0.0050,  0.0212,  0.0076]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_attention': tensor([[[0.0101, 0.1719, 0.0928,  ..., 0.0610, 0.0408, 0.0894],\n",
       "          [0.0095, 0.1709, 0.0923,  ..., 0.0615, 0.0398, 0.0894],\n",
       "          [0.0109, 0.1709, 0.0908,  ..., 0.0618, 0.0400, 0.0898]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_output': tensor([[[-19.3750, 121.5000,  -2.6875,  ..., -71.5000,   1.7812,  12.5000],\n",
       "          [-18.3750, 121.5000,  -3.6719,  ..., -74.5000,  -0.2500,   9.3125],\n",
       "          [-13.3750,  87.0000,  -4.4062,  ..., -60.7500,   0.5938,   7.0625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_attn_norm': tensor([[[-0.0167,  0.0532, -0.0028,  ..., -0.0356,  0.0008,  0.0089],\n",
       "          [-0.0156,  0.0525, -0.0038,  ..., -0.0366, -0.0001,  0.0066],\n",
       "          [-0.0145,  0.0474, -0.0057,  ..., -0.0376,  0.0003,  0.0063]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_q_proj': tensor([[[ 0.6289, -0.4473, -0.7734,  ...,  0.2285, -0.4199, -0.0874],\n",
       "          [ 0.6289, -0.4434, -0.7734,  ...,  0.2266, -0.4238, -0.0908],\n",
       "          [ 0.6367, -0.4473, -0.7734,  ...,  0.2324, -0.4277, -0.0923]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_k_proj': tensor([[[-0.0220,  2.0156, -0.1885,  ...,  0.2236, -0.0674,  0.1709],\n",
       "          [-0.0233,  2.0156, -0.1875,  ...,  0.2227, -0.0649,  0.1719],\n",
       "          [-0.0302,  2.0000, -0.1826,  ...,  0.2227, -0.0693,  0.1768]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_v_proj': tensor([[[ 0.0288,  0.0425, -0.0327,  ..., -0.0361, -0.0223,  0.0077],\n",
       "          [ 0.0294,  0.0410, -0.0344,  ..., -0.0364, -0.0209,  0.0075],\n",
       "          [ 0.0304,  0.0430, -0.0342,  ..., -0.0332, -0.0179,  0.0079]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_attention': tensor([[[-1.3794e-02, -1.6724e-02,  4.7302e-03,  ..., -4.8584e-02,\n",
       "           -7.5073e-03,  1.0400e-01],\n",
       "          [-1.3428e-02, -1.6602e-02,  2.4414e-03,  ..., -5.1025e-02,\n",
       "           -6.2866e-03,  1.0498e-01],\n",
       "          [-1.5625e-02, -1.6357e-02, -7.7486e-06,  ..., -5.1514e-02,\n",
       "           -5.4626e-03,  1.0400e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_output': tensor([[[-2.0375e+01,  1.1850e+02, -3.1562e+00,  ..., -7.0500e+01,\n",
       "            1.1406e+00,  1.2375e+01],\n",
       "          [-1.9375e+01,  1.1850e+02, -4.1250e+00,  ..., -7.3500e+01,\n",
       "           -8.7500e-01,  9.1250e+00],\n",
       "          [-1.4375e+01,  8.4000e+01, -4.9375e+00,  ..., -6.0000e+01,\n",
       "           -5.8594e-02,  6.9062e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_attn_norm': tensor([[[-1.5259e-02,  4.5410e-02, -3.7842e-03,  ..., -3.0640e-02,\n",
       "            3.8338e-04,  7.5073e-03],\n",
       "          [-1.4282e-02,  4.4922e-02, -4.8828e-03,  ..., -3.1494e-02,\n",
       "           -2.8801e-04,  5.4626e-03],\n",
       "          [-1.3367e-02,  4.0283e-02, -7.3242e-03,  ..., -3.2471e-02,\n",
       "           -2.4438e-05,  5.2185e-03]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_q_proj': tensor([[[-0.2373, -2.3906,  2.9062,  ...,  0.5352,  0.5117, -0.1099],\n",
       "          [-0.2373, -2.3906,  2.9062,  ...,  0.5312,  0.5156, -0.1123],\n",
       "          [-0.2363, -2.3906,  2.9062,  ...,  0.5312,  0.5234, -0.1172]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_k_proj': tensor([[[ 0.1260, -0.9023,  1.2969,  ...,  0.0410,  0.4199, -0.7461],\n",
       "          [ 0.1279, -0.9023,  1.2969,  ...,  0.0356,  0.4258, -0.7461],\n",
       "          [ 0.1226, -0.8945,  1.2969,  ...,  0.0192,  0.4199, -0.7422]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_v_proj': tensor([[[-0.0059, -0.0009, -0.0308,  ..., -0.0236,  0.0106, -0.0244],\n",
       "          [-0.0071, -0.0015, -0.0276,  ..., -0.0234,  0.0098, -0.0245],\n",
       "          [-0.0075, -0.0034, -0.0265,  ..., -0.0253,  0.0045, -0.0204]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_attention': tensor([[[-0.0464, -0.0425,  0.1631,  ..., -0.0129,  0.0571,  0.2188],\n",
       "          [-0.0481, -0.0413,  0.1641,  ..., -0.0104,  0.0564,  0.2197],\n",
       "          [-0.0469, -0.0376,  0.1631,  ..., -0.0109,  0.0525,  0.2197]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_output': tensor([[[-1.9875e+01,  1.1700e+02, -1.1719e+00,  ..., -7.0500e+01,\n",
       "            2.0000e+00,  1.2750e+01],\n",
       "          [-1.8875e+01,  1.1700e+02, -2.1250e+00,  ..., -7.3500e+01,\n",
       "           -1.1719e-02,  9.5000e+00],\n",
       "          [-1.4000e+01,  8.2500e+01, -2.9688e+00,  ..., -6.0250e+01,\n",
       "            8.5938e-01,  7.2500e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_attn_norm': tensor([[[-2.2705e-02,  5.6152e-02, -1.7853e-03,  ..., -4.2480e-02,\n",
       "            1.0834e-03,  1.0132e-02],\n",
       "          [-2.1240e-02,  5.5176e-02, -3.2043e-03,  ..., -4.3701e-02,\n",
       "           -6.2287e-06,  7.4463e-03],\n",
       "          [-1.9775e-02,  4.9072e-02, -5.6152e-03,  ..., -4.5166e-02,\n",
       "            5.7983e-04,  7.1716e-03]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_q_proj': tensor([[[-0.7812,  0.1162, -0.3320,  ..., -0.4219,  0.0957, -3.3281],\n",
       "          [-0.7852,  0.1147, -0.3320,  ..., -0.4199,  0.0977, -3.3281],\n",
       "          [-0.7812,  0.1143, -0.3379,  ..., -0.4160,  0.0923, -3.3438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_k_proj': tensor([[[-0.0752, -0.2051, -0.4336,  ..., -5.9062,  5.8125, -8.3125],\n",
       "          [-0.0757, -0.2080, -0.4355,  ..., -5.9062,  5.8125, -8.3125],\n",
       "          [-0.0752, -0.2080, -0.4297,  ..., -5.9688,  5.8125, -8.3125]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_v_proj': tensor([[[ 0.0515, -0.0322, -0.0159,  ..., -0.0157, -0.0014, -0.0139],\n",
       "          [ 0.0500, -0.0312, -0.0139,  ..., -0.0141, -0.0039, -0.0155],\n",
       "          [ 0.0471, -0.0308, -0.0146,  ..., -0.0146, -0.0047, -0.0156]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_attention': tensor([[[ 0.0786, -0.1055, -0.0601,  ..., -0.1309,  0.1924,  0.0708],\n",
       "          [ 0.0771, -0.1094, -0.0591,  ..., -0.1309,  0.1914,  0.0723],\n",
       "          [ 0.0762, -0.1108, -0.0610,  ..., -0.1289,  0.1895,  0.0713]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_output': tensor([[[-20.1250, 116.5000,  -0.9609,  ..., -71.5000,   2.2656,  12.0000],\n",
       "          [-19.1250, 116.5000,  -1.9219,  ..., -74.5000,   0.2617,   8.7500],\n",
       "          [-14.3125,  82.0000,  -2.7656,  ..., -61.5000,   1.1094,   6.5000]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_attn_norm': tensor([[[-0.0248,  0.0640, -0.0015,  ..., -0.0366,  0.0012,  0.0102],\n",
       "          [-0.0231,  0.0630, -0.0030,  ..., -0.0376,  0.0001,  0.0073],\n",
       "          [-0.0219,  0.0557, -0.0054,  ..., -0.0391,  0.0007,  0.0068]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_q_proj': tensor([[[ 0.0747,  0.1074, -0.1406,  ..., -0.8789,  0.2461,  0.2910],\n",
       "          [ 0.0713,  0.1074, -0.1406,  ..., -0.8828,  0.2432,  0.2910],\n",
       "          [ 0.0659,  0.1064, -0.1416,  ..., -0.8750,  0.2412,  0.2949]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_k_proj': tensor([[[ 0.1235,  0.2949, -0.4570,  ..., -0.6406,  0.0288,  0.3496],\n",
       "          [ 0.1245,  0.2930, -0.4570,  ..., -0.6406,  0.0260,  0.3496],\n",
       "          [ 0.1182,  0.3008, -0.4492,  ..., -0.6406,  0.0109,  0.3340]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_v_proj': tensor([[[-0.0469,  0.0040,  0.0991,  ..., -0.0097, -0.0258,  0.0227],\n",
       "          [-0.0474,  0.0020,  0.0977,  ..., -0.0087, -0.0258,  0.0232],\n",
       "          [-0.0459,  0.0080,  0.0947,  ..., -0.0096, -0.0216,  0.0233]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_attention': tensor([[[-0.1406, -0.0630,  0.0356,  ..., -0.0050,  0.1514, -0.0030],\n",
       "          [-0.1445, -0.0654,  0.0374,  ..., -0.0042,  0.1523, -0.0006],\n",
       "          [-0.1445, -0.0684,  0.0383,  ..., -0.0036,  0.1514, -0.0006]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_output': tensor([[[-20.8750, 117.0000,  -1.3438,  ..., -71.5000,   2.2031,  11.3750],\n",
       "          [-19.8750, 117.0000,  -2.2969,  ..., -74.5000,   0.2002,   8.1250],\n",
       "          [-15.0625,  82.5000,  -3.1406,  ..., -61.7500,   1.0391,   5.9062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_attn_norm': tensor([[[-0.0344,  0.0781, -0.0028,  ..., -0.0339,  0.0017,  0.0125],\n",
       "          [-0.0322,  0.0776, -0.0047,  ..., -0.0349,  0.0002,  0.0088],\n",
       "          [-0.0308,  0.0684, -0.0081,  ..., -0.0364,  0.0010,  0.0080]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_q_proj': tensor([[[ 0.4336, -0.0204,  0.1104,  ...,  0.3086, -0.2734,  0.4082],\n",
       "          [ 0.4375, -0.0171,  0.1118,  ...,  0.3086, -0.2754,  0.4121],\n",
       "          [ 0.4414, -0.0134,  0.1025,  ...,  0.3047, -0.2617,  0.4141]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_k_proj': tensor([[[ 3.2959e-03, -2.2656e+00,  1.6172e+00,  ...,  7.7344e-01,\n",
       "            5.4062e+00,  1.7812e+00],\n",
       "          [ 3.0670e-03, -2.2656e+00,  1.6172e+00,  ...,  7.7344e-01,\n",
       "            5.4062e+00,  1.7891e+00],\n",
       "          [ 1.5503e-02, -2.2656e+00,  1.6094e+00,  ...,  7.6172e-01,\n",
       "            5.4062e+00,  1.7969e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_v_proj': tensor([[[ 0.0376,  0.0549, -0.0245,  ..., -0.0126,  0.0236,  0.0374],\n",
       "          [ 0.0364,  0.0544, -0.0266,  ..., -0.0101,  0.0212,  0.0405],\n",
       "          [ 0.0337,  0.0549, -0.0260,  ..., -0.0134,  0.0215,  0.0400]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_attention': tensor([[[ 0.1475, -0.1699, -0.0248,  ...,  0.0059, -0.0356, -0.0991],\n",
       "          [ 0.1484, -0.1699, -0.0247,  ...,  0.0034, -0.0359, -0.1021],\n",
       "          [ 0.1416, -0.1689, -0.0266,  ..., -0.0008, -0.0342, -0.1035]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_output': tensor([[[-21.8750, 116.0000,  -4.0312,  ..., -70.5000,   3.7500,   9.1250],\n",
       "          [-20.8750, 116.0000,  -5.0000,  ..., -73.5000,   1.7578,   5.8438],\n",
       "          [-16.1250,  81.5000,  -5.7500,  ..., -60.7500,   2.6562,   3.5781]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_attn_norm': tensor([[[-0.0315,  0.0708, -0.0086,  ..., -0.0356,  0.0029,  0.0089],\n",
       "          [-0.0297,  0.0703, -0.0105,  ..., -0.0366,  0.0013,  0.0056],\n",
       "          [-0.0288,  0.0618, -0.0152,  ..., -0.0378,  0.0025,  0.0043]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_q_proj': tensor([[[0.3301, 0.0107, 0.0747,  ..., 0.3301, 0.3945, 0.0381],\n",
       "          [0.3320, 0.0131, 0.0757,  ..., 0.3262, 0.3945, 0.0403],\n",
       "          [0.3418, 0.0058, 0.0708,  ..., 0.3242, 0.3926, 0.0288]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_k_proj': tensor([[[ 0.0737,  0.6992, -0.7383,  ..., -0.8047, -0.1562, -0.3105],\n",
       "          [ 0.0806,  0.6992, -0.7422,  ..., -0.8086, -0.1572, -0.3066],\n",
       "          [ 0.0762,  0.7031, -0.7461,  ..., -0.8242, -0.1602, -0.3105]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_v_proj': tensor([[[ 0.0225, -0.0117, -0.0613,  ...,  0.0248,  0.0183,  0.0272],\n",
       "          [ 0.0227, -0.0110, -0.0605,  ...,  0.0254,  0.0210,  0.0287],\n",
       "          [ 0.0093, -0.0161, -0.0654,  ...,  0.0226,  0.0204,  0.0312]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_attention': tensor([[[ 0.0688, -0.0559, -0.1504,  ..., -0.2070, -0.1396, -0.0996],\n",
       "          [ 0.0669, -0.0566, -0.1484,  ..., -0.2061, -0.1377, -0.1016],\n",
       "          [ 0.0664, -0.0588, -0.1445,  ..., -0.2031, -0.1357, -0.1021]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_output': tensor([[[-21.8750, 115.5000,  -5.4375,  ..., -70.0000,   3.4219,   9.2500],\n",
       "          [-20.8750, 115.5000,  -6.4062,  ..., -73.0000,   1.4297,   6.0000],\n",
       "          [-16.1250,  81.0000,  -7.1250,  ..., -60.2500,   2.3438,   3.7031]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_attn_norm': tensor([[[-0.0339,  0.0713, -0.0123,  ..., -0.0315,  0.0026,  0.0089],\n",
       "          [-0.0320,  0.0703, -0.0143,  ..., -0.0325,  0.0011,  0.0057],\n",
       "          [-0.0310,  0.0620, -0.0199,  ..., -0.0334,  0.0022,  0.0044]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_q_proj': tensor([[[-0.0184,  0.4395,  0.5859,  ..., -0.0271, -0.7734, -0.0312],\n",
       "          [-0.0188,  0.4395,  0.5898,  ..., -0.0266, -0.7734, -0.0332],\n",
       "          [-0.0139,  0.4395,  0.5938,  ..., -0.0244, -0.7773, -0.0366]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_k_proj': tensor([[[ 0.0120,  1.5156, -0.0156,  ..., -3.3594,  1.8125,  0.1689],\n",
       "          [ 0.0101,  1.5234, -0.0125,  ..., -3.3594,  1.8047,  0.1699],\n",
       "          [ 0.0097,  1.5234, -0.0182,  ..., -3.3750,  1.8047,  0.1719]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_v_proj': tensor([[[ 0.0242,  0.1099,  0.0135,  ..., -0.0281,  0.0161,  0.0057],\n",
       "          [ 0.0236,  0.1099,  0.0126,  ..., -0.0278,  0.0172,  0.0092],\n",
       "          [ 0.0264,  0.1123,  0.0111,  ..., -0.0239,  0.0211,  0.0093]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_attention': tensor([[[ 0.0181,  0.3184, -0.2061,  ..., -0.3203, -0.1475,  0.1289],\n",
       "          [ 0.0156,  0.3223, -0.2178,  ..., -0.3184, -0.1465,  0.1299],\n",
       "          [ 0.0104,  0.3242, -0.2080,  ..., -0.3145, -0.1406,  0.1260]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_output': tensor([[[-22.1250, 116.0000,  -5.5625,  ..., -69.5000,   3.4219,   9.5000],\n",
       "          [-21.1250, 116.0000,  -6.5312,  ..., -72.5000,   1.4219,   6.2188],\n",
       "          [-16.3750,  81.5000,  -7.2812,  ..., -59.5000,   2.3125,   3.9219]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_attn_norm': tensor([[[-0.0422,  0.0835, -0.0140,  ..., -0.0254,  0.0035,  0.0121],\n",
       "          [-0.0398,  0.0825, -0.0164,  ..., -0.0262,  0.0015,  0.0078],\n",
       "          [-0.0383,  0.0723, -0.0226,  ..., -0.0267,  0.0030,  0.0062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_q_proj': tensor([[[-7.0625e+00,  8.9453e-01, -1.9922e+00,  ...,  6.9922e-01,\n",
       "           -7.0801e-02,  4.6692e-03],\n",
       "          [-7.0312e+00,  8.9453e-01, -1.9922e+00,  ...,  6.9141e-01,\n",
       "           -6.7383e-02, -4.8828e-03],\n",
       "          [-7.0625e+00,  8.9844e-01, -1.9844e+00,  ...,  6.8359e-01,\n",
       "           -6.8848e-02,  2.1210e-03]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_k_proj': tensor([[[-0.0020, -0.5664, -0.3887,  ...,  0.0188, -0.0466,  0.9141],\n",
       "          [ 0.0019, -0.5703, -0.3867,  ...,  0.0157, -0.0417,  0.9102],\n",
       "          [-0.0019, -0.5664, -0.3887,  ...,  0.0046, -0.0452,  0.9102]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_v_proj': tensor([[[ 0.0089, -0.0033,  0.0153,  ..., -0.0378,  0.0013,  0.0084],\n",
       "          [ 0.0094, -0.0042,  0.0193,  ..., -0.0393,  0.0029,  0.0088],\n",
       "          [ 0.0177, -0.0073,  0.0173,  ..., -0.0356,  0.0050, -0.0002]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_attention': tensor([[[-0.3438,  0.0942,  0.3574,  ..., -0.4062, -0.2598,  0.0204],\n",
       "          [-0.3438,  0.0938,  0.3594,  ..., -0.4043, -0.2578,  0.0194],\n",
       "          [-0.3438,  0.0859,  0.3594,  ..., -0.4004, -0.2598,  0.0176]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_output': tensor([[[-23.0000, 115.5000,  -5.1562,  ..., -69.0000,   2.8281,   9.5000],\n",
       "          [-22.1250, 115.5000,  -6.1250,  ..., -72.0000,   0.8398,   6.2812],\n",
       "          [-17.3750,  81.0000,  -6.8438,  ..., -59.2500,   1.7031,   3.9531]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_attn_norm': tensor([[[-0.0464,  0.0972, -0.0161,  ..., -0.0255,  0.0036,  0.0132],\n",
       "          [-0.0442,  0.0962, -0.0189,  ..., -0.0264,  0.0010,  0.0086],\n",
       "          [-0.0432,  0.0840, -0.0264,  ..., -0.0270,  0.0027,  0.0068]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_q_proj': tensor([[[-0.9414,  1.1016,  0.0596,  ...,  0.0908,  0.2090, -0.9180],\n",
       "          [-0.9453,  1.1016,  0.0576,  ...,  0.0933,  0.2080, -0.9180],\n",
       "          [-0.9492,  1.0859,  0.0522,  ...,  0.0933,  0.2207, -0.9180]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_k_proj': tensor([[[-0.0342,  0.0432,  0.2002,  ...,  1.8984,  1.8281,  3.3906],\n",
       "          [-0.0344,  0.0439,  0.2041,  ...,  1.8906,  1.8281,  3.3906],\n",
       "          [-0.0352,  0.0361,  0.2061,  ...,  1.8984,  1.8359,  3.4062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_v_proj': tensor([[[-0.0138, -0.0162,  0.0171,  ..., -0.0131, -0.0522,  0.0162],\n",
       "          [-0.0144, -0.0114,  0.0157,  ..., -0.0109, -0.0503,  0.0118],\n",
       "          [-0.0153, -0.0132,  0.0143,  ..., -0.0063, -0.0518,  0.0128]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_attention': tensor([[[-0.2637, -0.0708, -1.9688,  ..., -0.1553, -0.1016,  0.1582],\n",
       "          [-0.2637, -0.0693, -1.9688,  ..., -0.1562, -0.1045,  0.1611],\n",
       "          [-0.2676, -0.0654, -1.9688,  ..., -0.1553, -0.1074,  0.1611]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_output': tensor([[[-23.1250, 115.0000,  -5.6250,  ..., -68.5000,   2.7031,  10.3125],\n",
       "          [-22.2500, 115.0000,  -6.6250,  ..., -71.5000,   0.7188,   7.0625],\n",
       "          [-17.5000,  80.5000,  -7.3125,  ..., -59.0000,   1.5547,   4.7812]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_attn_norm': tensor([[[-0.0500,  0.0933, -0.0164,  ..., -0.0295,  0.0035,  0.0141],\n",
       "          [-0.0476,  0.0918, -0.0189,  ..., -0.0305,  0.0009,  0.0095],\n",
       "          [-0.0466,  0.0806, -0.0261,  ..., -0.0315,  0.0025,  0.0080]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_q_proj': tensor([[[ 0.1240, -0.1738,  0.2393,  ...,  1.2266, -1.2266,  0.1245],\n",
       "          [ 0.1216, -0.1738,  0.2373,  ...,  1.2188, -1.2344,  0.1279],\n",
       "          [ 0.1187, -0.1729,  0.2236,  ...,  1.2188, -1.2266,  0.1250]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_k_proj': tensor([[[-0.1089,  0.1289, -0.3301,  ...,  0.3691,  0.6992, -2.2812],\n",
       "          [-0.1113,  0.1270, -0.3301,  ...,  0.3789,  0.6992, -2.2812],\n",
       "          [-0.1064,  0.1318, -0.3320,  ...,  0.3828,  0.7148, -2.2812]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_v_proj': tensor([[[ 0.0471,  0.0096, -0.0398,  ...,  0.0309,  0.0299,  0.0045],\n",
       "          [ 0.0459,  0.0070, -0.0391,  ...,  0.0330,  0.0320,  0.0058],\n",
       "          [ 0.0469,  0.0032, -0.0359,  ...,  0.0306,  0.0337,  0.0028]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_attention': tensor([[[ 0.2402,  0.1270, -0.0840,  ..., -0.2139, -0.4648, -0.3984],\n",
       "          [ 0.2373,  0.1289, -0.0815,  ..., -0.2129, -0.4629, -0.3984],\n",
       "          [ 0.2314,  0.1328, -0.0762,  ..., -0.2158, -0.4668, -0.4004]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_output': tensor([[[-22.6250, 114.5000,  -5.9062,  ..., -68.0000,   2.4688,  10.1250],\n",
       "          [-21.7500, 114.5000,  -6.9062,  ..., -71.0000,   0.4961,   6.8750],\n",
       "          [-17.0000,  80.0000,  -7.5625,  ..., -59.0000,   1.3203,   4.5938]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_attn_norm': tensor([[[-0.0439,  0.0742, -0.0171,  ..., -0.0238,  0.0031,  0.0131],\n",
       "          [-0.0417,  0.0737, -0.0197,  ..., -0.0245,  0.0006,  0.0089],\n",
       "          [-0.0408,  0.0640, -0.0269,  ..., -0.0254,  0.0021,  0.0073]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_q_proj': tensor([[[ 0.5625, -1.5312, -0.2773,  ...,  0.0435, -0.3789,  0.1973],\n",
       "          [ 0.5586, -1.5312, -0.2812,  ...,  0.0466, -0.3809,  0.1953],\n",
       "          [ 0.5547, -1.5156, -0.2832,  ...,  0.0664, -0.3848,  0.1992]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_k_proj': tensor([[[-0.0234,  0.4316, -1.2109,  ...,  1.3828,  2.8906, -0.7148],\n",
       "          [-0.0253,  0.4336, -1.2109,  ...,  1.3828,  2.8906, -0.7188],\n",
       "          [-0.0304,  0.4277, -1.2109,  ...,  1.3750,  2.9062, -0.7070]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_v_proj': tensor([[[-0.1235, -0.0522,  0.0349,  ..., -0.0131,  0.0120,  0.0659],\n",
       "          [-0.1260, -0.0544,  0.0356,  ..., -0.0134,  0.0139,  0.0679],\n",
       "          [-0.1226, -0.0557,  0.0369,  ..., -0.0228,  0.0280,  0.0664]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_attention': tensor([[[-0.0540,  0.1084, -0.0037,  ..., -0.0364,  0.2139,  0.2930],\n",
       "          [-0.0583,  0.1060, -0.0024,  ..., -0.0342,  0.2168,  0.2891],\n",
       "          [-0.0576,  0.1050, -0.0003,  ..., -0.0293,  0.2148,  0.2852]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_output': tensor([[[-22.7500, 114.0000,  -5.3438,  ..., -67.5000,   2.7344,  10.1875],\n",
       "          [-21.8750, 114.0000,  -6.3125,  ..., -70.5000,   0.7539,   6.9375],\n",
       "          [-17.1250,  79.5000,  -6.9688,  ..., -58.5000,   1.5781,   4.6250]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_attn_norm': tensor([[[-0.0522,  0.1040, -0.0179,  ..., -0.0292,  0.0042,  0.0175],\n",
       "          [-0.0498,  0.1025, -0.0209,  ..., -0.0300,  0.0012,  0.0117],\n",
       "          [-0.0486,  0.0894, -0.0286,  ..., -0.0311,  0.0030,  0.0098]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_q_proj': tensor([[[ 0.3535, -0.2734,  0.4844,  ...,  0.2236,  0.1426,  0.8086],\n",
       "          [ 0.3555, -0.2695,  0.4824,  ...,  0.2119,  0.1396,  0.8008],\n",
       "          [ 0.3652, -0.2656,  0.4863,  ...,  0.2275,  0.1406,  0.7852]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_k_proj': tensor([[[ 0.1113,  0.2832, -0.7344,  ..., -1.4922, -0.3711,  0.4844],\n",
       "          [ 0.1133,  0.2832, -0.7344,  ..., -1.4922, -0.3672,  0.4824],\n",
       "          [ 0.1040,  0.2852, -0.7305,  ..., -1.4844, -0.3535,  0.4824]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_v_proj': tensor([[[ 0.0703,  0.6797,  0.0393,  ...,  0.0261, -0.0211,  0.0189],\n",
       "          [ 0.0679,  0.6836,  0.0408,  ...,  0.0245, -0.0204,  0.0182],\n",
       "          [ 0.0610,  0.6875,  0.0347,  ...,  0.0249, -0.0182,  0.0206]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_attention': tensor([[[ 0.2539, -0.2734,  0.1836,  ..., -0.2412, -0.2031,  0.0483],\n",
       "          [ 0.2559, -0.2754,  0.1836,  ..., -0.2402, -0.2012,  0.0505],\n",
       "          [ 0.2559, -0.2754,  0.1865,  ..., -0.2393, -0.2002,  0.0491]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_output': tensor([[[-22.6250, 113.5000,  -4.7500,  ..., -67.0000,   2.6094,  10.0625],\n",
       "          [-21.7500, 113.5000,  -5.7188,  ..., -70.0000,   0.6172,   6.7812],\n",
       "          [-17.0000,  79.0000,  -6.3750,  ..., -58.2500,   1.4297,   4.4688]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_attn_norm': tensor([[[-0.0530,  0.1089, -0.0192,  ..., -0.0322,  0.0046,  0.0176],\n",
       "          [-0.0503,  0.1074, -0.0228,  ..., -0.0334,  0.0011,  0.0117],\n",
       "          [-0.0493,  0.0933, -0.0317,  ..., -0.0347,  0.0031,  0.0096]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_q_proj': tensor([[[ 0.7422, -0.2676, -0.1279,  ..., -0.6445,  0.0065,  0.2363],\n",
       "          [ 0.7422, -0.2656, -0.1289,  ..., -0.6445,  0.0041,  0.2354],\n",
       "          [ 0.7500, -0.2539, -0.1182,  ..., -0.6562,  0.0079,  0.2314]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_k_proj': tensor([[[-0.0742,  0.4531, -0.7031,  ..., -2.3281,  1.0547,  1.7422],\n",
       "          [-0.0737,  0.4531, -0.7070,  ..., -2.3281,  1.0547,  1.7422],\n",
       "          [-0.0645,  0.4512, -0.7031,  ..., -2.3438,  1.0703,  1.7422]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_v_proj': tensor([[[-0.1260, -0.0361, -0.0150,  ..., -0.0127,  0.0084,  0.1025],\n",
       "          [-0.1240, -0.0400, -0.0162,  ..., -0.0109,  0.0106,  0.1060],\n",
       "          [-0.1201, -0.0400, -0.0102,  ..., -0.0066,  0.0171,  0.1113]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_attention': tensor([[[ 0.3418,  0.0815, -0.1895,  ..., -0.6445,  0.2676,  0.3145],\n",
       "          [ 0.3418,  0.0845, -0.1904,  ..., -0.6406,  0.2676,  0.3125],\n",
       "          [ 0.3418,  0.0894, -0.1943,  ..., -0.6406,  0.2676,  0.3086]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_output': tensor([[[-22.5000, 113.5000,  -5.1875,  ..., -67.0000,   3.0938,  10.5625],\n",
       "          [-21.6250, 113.5000,  -6.1562,  ..., -70.0000,   1.1094,   7.2500],\n",
       "          [-16.8750,  79.0000,  -6.8125,  ..., -58.7500,   1.9062,   4.9375]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_attn_norm': tensor([[[-0.0466,  0.1260, -0.0184,  ..., -0.0315,  0.0066,  0.0203],\n",
       "          [-0.0439,  0.1250, -0.0216,  ..., -0.0325,  0.0023,  0.0137],\n",
       "          [-0.0430,  0.1084, -0.0297,  ..., -0.0339,  0.0050,  0.0117]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_q_proj': tensor([[[ 1.2266e+00, -2.9844e+00,  4.5312e-01,  ..., -1.4531e+00,\n",
       "            1.0000e+00,  1.2131e-03],\n",
       "          [ 1.2188e+00, -2.9844e+00,  4.5508e-01,  ..., -1.4531e+00,\n",
       "            1.0000e+00, -1.5450e-04],\n",
       "          [ 1.2344e+00, -2.9531e+00,  4.4922e-01,  ..., -1.4531e+00,\n",
       "            9.8828e-01,  9.7046e-03]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_k_proj': tensor([[[ 0.0339, -0.9023,  0.3477,  ...,  0.8711,  0.1641,  2.6406],\n",
       "          [ 0.0342, -0.9023,  0.3477,  ...,  0.8711,  0.1670,  2.6406],\n",
       "          [ 0.0287, -0.8984,  0.3418,  ...,  0.8750,  0.1738,  2.6562]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_v_proj': tensor([[[-0.1045,  0.0649,  0.1348,  ...,  0.0171, -0.0125,  0.0942],\n",
       "          [-0.1035,  0.0635,  0.1396,  ...,  0.0200, -0.0118,  0.0933],\n",
       "          [-0.1025,  0.0598,  0.1318,  ...,  0.0284, -0.0113,  0.0972]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_attention': tensor([[[ 0.3867, -0.2773, -0.4746,  ..., -0.1377, -0.1201, -0.0718],\n",
       "          [ 0.3848, -0.2812, -0.4766,  ..., -0.1396, -0.1162, -0.0723],\n",
       "          [ 0.3828, -0.2793, -0.4688,  ..., -0.1367, -0.1157, -0.0703]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_output': tensor([[[-21.6250, 113.0000,  -4.4062,  ..., -66.5000,   2.8125,  10.6875],\n",
       "          [-20.7500, 113.0000,  -5.3750,  ..., -69.5000,   0.8242,   7.4062],\n",
       "          [-16.0000,  78.5000,  -6.0625,  ..., -58.5000,   1.6250,   5.0625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_attn_norm': tensor([[[-0.0422,  0.1152, -0.0116,  ..., -0.0481,  0.0042,  0.0162],\n",
       "          [-0.0400,  0.1143, -0.0140,  ..., -0.0496,  0.0012,  0.0111],\n",
       "          [-0.0386,  0.0991, -0.0197,  ..., -0.0522,  0.0030,  0.0095]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_q_proj': tensor([[[-53.2500, -10.2500, -30.7500,  ...,  -2.7031,   0.2871,   0.3770],\n",
       "          [-53.2500, -10.2500, -30.7500,  ...,  -2.6875,   0.2852,   0.3789],\n",
       "          [-53.2500, -10.2500, -30.7500,  ...,  -2.6719,   0.2852,   0.3828]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_k_proj': tensor([[[-1.2436e-03, -1.1328e+00,  3.5889e-02,  ...,  6.4844e-01,\n",
       "            8.0625e+00,  5.5859e-01],\n",
       "          [ 4.8828e-04, -1.1328e+00,  3.1738e-02,  ...,  6.4453e-01,\n",
       "            8.0625e+00,  5.5859e-01],\n",
       "          [-6.9580e-03, -1.1328e+00,  3.2715e-02,  ...,  6.0938e-01,\n",
       "            8.0625e+00,  5.6250e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_v_proj': tensor([[[ 0.0830, -0.1260,  0.0603,  ..., -0.0688,  0.1445, -0.0082],\n",
       "          [ 0.0854, -0.1240,  0.0713,  ..., -0.0688,  0.1475, -0.0082],\n",
       "          [ 0.0825, -0.0967,  0.0723,  ..., -0.0713,  0.1445, -0.0062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_attention': tensor([[[-0.3008, -0.8125,  0.0198,  ...,  0.0981,  0.0791,  0.3418],\n",
       "          [-0.3008, -0.8125,  0.0153,  ...,  0.0894,  0.0825,  0.3398],\n",
       "          [-0.3027, -0.8086,  0.0142,  ...,  0.0967,  0.0757,  0.3398]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_output': tensor([[[-21.7500, 111.5000,  -3.0156,  ..., -66.5000,   2.4219,  10.8750],\n",
       "          [-20.8750, 111.5000,  -4.0000,  ..., -69.5000,   0.4355,   7.6562],\n",
       "          [-16.1250,  77.0000,  -4.7500,  ..., -58.2500,   1.2188,   5.3125]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_attn_norm': tensor([[[-0.0381,  0.1177, -0.0074,  ..., -0.0515,  0.0037,  0.0153],\n",
       "          [-0.0361,  0.1162, -0.0097,  ..., -0.0530,  0.0007,  0.0106],\n",
       "          [-0.0347,  0.1001, -0.0143,  ..., -0.0557,  0.0023,  0.0092]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_q_proj': tensor([[[ 0.8633,  0.4492, -1.3516,  ...,  0.7188, -0.1436,  0.8203],\n",
       "          [ 0.8594,  0.4453, -1.3594,  ...,  0.7148, -0.1416,  0.8203],\n",
       "          [ 0.8516,  0.4531, -1.3516,  ...,  0.7148, -0.1436,  0.8164]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_k_proj': tensor([[[-0.1235, -1.4297, -0.6016,  ..., -0.3242,  0.9609,  0.3262],\n",
       "          [-0.1216, -1.4375, -0.5977,  ..., -0.3242,  0.9688,  0.3262],\n",
       "          [-0.1221, -1.4375, -0.6055,  ..., -0.3223,  0.9531,  0.3301]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_v_proj': tensor([[[ 0.0245, -0.1514, -0.1099,  ...,  0.0913, -0.0311, -0.0718],\n",
       "          [ 0.0259, -0.1504, -0.1084,  ...,  0.0913, -0.0322, -0.0732],\n",
       "          [ 0.0352, -0.1494, -0.1094,  ...,  0.0933, -0.0483, -0.0918]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_attention': tensor([[[-0.4707, -0.5430, -1.1797,  ...,  0.1377, -0.5078,  0.0292],\n",
       "          [-0.4746, -0.5469, -1.1797,  ...,  0.1377, -0.5117,  0.0250],\n",
       "          [-0.4668, -0.5430, -1.1797,  ...,  0.1367, -0.5156,  0.0305]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_output': tensor([[[-22.1250, 110.0000,  -4.1250,  ..., -66.5000,   1.0078,  10.6875],\n",
       "          [-21.2500, 110.0000,  -5.0938,  ..., -69.5000,  -0.9688,   7.5000],\n",
       "          [-16.5000,  75.5000,  -5.8438,  ..., -58.0000,  -0.1523,   5.1562]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_attn_norm': tensor([[[-0.0322,  0.1011, -0.0068,  ..., -0.0806,  0.0012,  0.0121],\n",
       "          [-0.0305,  0.1001, -0.0083,  ..., -0.0830, -0.0011,  0.0084],\n",
       "          [-0.0295,  0.0850, -0.0119,  ..., -0.0864, -0.0002,  0.0072]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_q_proj': tensor([[[-0.0747, -0.1357, -0.6250,  ...,  0.6523,  0.6016,  1.4609],\n",
       "          [-0.0781, -0.1348, -0.6250,  ...,  0.6445,  0.6055,  1.4609],\n",
       "          [-0.0762, -0.1191, -0.6133,  ...,  0.6445,  0.6016,  1.4453]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_k_proj': tensor([[[ 0.0825, -0.7930,  0.6992,  ..., -0.1416,  0.8867, -0.2139],\n",
       "          [ 0.0786, -0.7891,  0.6953,  ..., -0.1445,  0.8789, -0.2119],\n",
       "          [ 0.0752, -0.7812,  0.6953,  ..., -0.1250,  0.8867, -0.2383]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_v_proj': tensor([[[ 0.0085,  0.1826,  0.0791,  ..., -0.1396,  0.0317,  0.0028],\n",
       "          [ 0.0058,  0.1797,  0.0811,  ..., -0.1416,  0.0281, -0.0012],\n",
       "          [ 0.0090,  0.1816,  0.0845,  ..., -0.1406,  0.0256, -0.0098]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_attention': tensor([[[ 0.4316, -0.2598,  0.9609,  ..., -0.6406,  0.3125, -0.3457],\n",
       "          [ 0.4297, -0.2598,  0.9609,  ..., -0.6406,  0.3105, -0.3438],\n",
       "          [ 0.4316, -0.2598,  0.9609,  ..., -0.6289,  0.2969, -0.3516]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_output': tensor([[[-22.7500, 108.5000,  -4.1250,  ..., -67.0000,   0.6055,   8.2500],\n",
       "          [-21.8750, 108.5000,  -5.0938,  ..., -70.0000,  -1.3906,   5.0625],\n",
       "          [-17.1250,  74.0000,  -5.8438,  ..., -58.7500,  -0.6055,   2.6875]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_attn_norm': tensor([[[-0.0315,  0.1021, -0.0056,  ..., -0.0728,  0.0007,  0.0093],\n",
       "          [-0.0300,  0.1006, -0.0069,  ..., -0.0752, -0.0016,  0.0057],\n",
       "          [-0.0293,  0.0854, -0.0099,  ..., -0.0791, -0.0009,  0.0037]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_q_proj': tensor([[[-0.0056,  0.4941,  0.5938,  ..., -0.2197, -0.0962, -0.4023],\n",
       "          [ 0.0019,  0.4922,  0.5977,  ..., -0.2129, -0.0996, -0.4023],\n",
       "          [ 0.0129,  0.4648,  0.6055,  ..., -0.2188, -0.0981, -0.4199]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_k_proj': tensor([[[-0.1279, -0.4043, -0.0073,  ...,  2.5625,  0.1680,  0.2070],\n",
       "          [-0.1309, -0.4023, -0.0060,  ...,  2.5625,  0.1709,  0.2051],\n",
       "          [-0.1235, -0.3965, -0.0110,  ...,  2.5625,  0.1689,  0.1748]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_v_proj': tensor([[[ 0.0481,  0.0938,  0.0125,  ...,  0.0214, -0.0903, -0.2158],\n",
       "          [ 0.0432,  0.0938,  0.0123,  ...,  0.0215, -0.0923, -0.2217],\n",
       "          [ 0.0354,  0.0835,  0.0104,  ...,  0.0232, -0.0874, -0.2178]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_attention': tensor([[[ 0.0903,  0.0742,  0.3770,  ..., -0.2031, -0.4219, -0.4902],\n",
       "          [ 0.0825,  0.0801,  0.3770,  ..., -0.2002, -0.4258, -0.4883],\n",
       "          [ 0.0771,  0.0815,  0.3809,  ..., -0.2031, -0.4277, -0.4883]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_output': tensor([[[-25.1250, 106.5000,  -4.3125,  ..., -67.5000,  -1.5781,   7.0312],\n",
       "          [-24.2500, 106.5000,  -5.2500,  ..., -70.5000,  -3.5625,   3.8438],\n",
       "          [-19.6250,  72.0000,  -6.0000,  ..., -59.7500,  -2.7812,   1.4219]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_attn_norm': tensor([[[-0.0289,  0.0903, -0.0048,  ..., -0.0635, -0.0015,  0.0069],\n",
       "          [-0.0276,  0.0894, -0.0058,  ..., -0.0659, -0.0034,  0.0037],\n",
       "          [-0.0278,  0.0752, -0.0082,  ..., -0.0693, -0.0033,  0.0017]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_q_proj': tensor([[[ 0.5352, -0.1069,  0.0957,  ..., -0.5391, -0.4902, -0.1289],\n",
       "          [ 0.5352, -0.1128,  0.0952,  ..., -0.5352, -0.4941, -0.1357],\n",
       "          [ 0.5273, -0.1201,  0.1055,  ..., -0.5430, -0.4746, -0.1338]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_k_proj': tensor([[[ 0.0830,  0.2109, -0.1050,  ..., -0.2070,  0.1904,  0.6602],\n",
       "          [ 0.0796,  0.2070, -0.0996,  ..., -0.2139,  0.1943,  0.6562],\n",
       "          [ 0.0820,  0.2041, -0.0972,  ..., -0.2139,  0.2197,  0.6562]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_v_proj': tensor([[[-0.0752,  0.1060, -0.0164,  ...,  0.2578,  0.0488, -0.1045],\n",
       "          [-0.0757,  0.1030, -0.0178,  ...,  0.2578,  0.0476, -0.1113],\n",
       "          [-0.0688,  0.0981, -0.0261,  ...,  0.2500,  0.0488, -0.1113]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_attention': tensor([[[-0.0233,  0.6523,  0.2676,  ...,  0.6523, -0.7383,  0.4941],\n",
       "          [-0.0234,  0.6523,  0.2656,  ...,  0.6445, -0.7383,  0.4961],\n",
       "          [-0.0201,  0.6602,  0.2695,  ...,  0.6445, -0.7422,  0.4922]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_output': tensor([[[-27.3750, 105.0000,  -3.9375,  ..., -68.0000,  -5.6875,   6.1875],\n",
       "          [-26.5000, 105.0000,  -4.9375,  ..., -71.0000,  -7.7188,   2.9375],\n",
       "          [-21.8750,  70.5000,  -5.7188,  ..., -59.7500,  -6.8750,   0.5938]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_attn_norm': tensor([[[-0.0332,  0.1060, -0.0050,  ..., -0.0723, -0.0064,  0.0071],\n",
       "          [-0.0320,  0.1045, -0.0063,  ..., -0.0747, -0.0086,  0.0033],\n",
       "          [-0.0330,  0.0879, -0.0090,  ..., -0.0781, -0.0096,  0.0008]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_q_proj': tensor([[[-2.7969,  0.9414,  0.6250,  ...,  0.2471,  1.6406,  0.9023],\n",
       "          [-2.7969,  0.9453,  0.6172,  ...,  0.2480,  1.6406,  0.9141],\n",
       "          [-2.7812,  0.9336,  0.6250,  ...,  0.2402,  1.6641,  0.8867]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_k_proj': tensor([[[-0.0552,  0.7031, -0.7617,  ...,  4.5312, -1.9062, -0.5898],\n",
       "          [-0.0613,  0.7031, -0.7656,  ...,  4.5625, -1.9062, -0.5742],\n",
       "          [-0.0564,  0.7031, -0.7734,  ...,  4.5312, -1.8906, -0.5703]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_v_proj': tensor([[[ 0.1660, -0.0447, -0.1582,  ..., -0.0505,  0.0211,  0.2598],\n",
       "          [ 0.1709, -0.0457, -0.1533,  ..., -0.0508,  0.0090,  0.2637],\n",
       "          [ 0.1650, -0.0630, -0.1611,  ..., -0.0645,  0.0016,  0.2578]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_attention': tensor([[[ 0.8516,  0.5977,  1.2891,  ...,  0.7500,  0.4531, -0.8750],\n",
       "          [ 0.8516,  0.5938,  1.2891,  ...,  0.7422,  0.4629, -0.8672],\n",
       "          [ 0.8516,  0.5938,  1.2734,  ...,  0.7422,  0.4609, -0.8633]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_output': tensor([[[-28.8750, 100.0000,  -1.5156,  ..., -67.0000,  -9.8125,   1.8438],\n",
       "          [-28.0000, 100.0000,  -2.5625,  ..., -70.5000, -11.8750,  -1.4219],\n",
       "          [-23.5000,  65.5000,  -3.3438,  ..., -58.7500, -10.8750,  -3.5625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_attn_norm': tensor([[[-0.0354,  0.1011, -0.0019,  ..., -0.0723, -0.0112,  0.0020],\n",
       "          [-0.0339,  0.1001, -0.0031,  ..., -0.0757, -0.0134, -0.0015],\n",
       "          [-0.0354,  0.0820, -0.0051,  ..., -0.0781, -0.0153, -0.0047]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_q_proj': tensor([[[ 0.6680,  0.7812,  0.2090,  ..., -1.0391, -4.8125, -2.6562],\n",
       "          [ 0.6641,  0.7852,  0.2051,  ..., -1.0391, -4.8125, -2.6562],\n",
       "          [ 0.6680,  0.7930,  0.2041,  ..., -1.0078, -4.7812, -2.6406]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_k_proj': tensor([[[-0.0571,  0.1982, -0.4844,  ..., -0.7891,  0.3438,  1.0312],\n",
       "          [-0.0598,  0.1973, -0.4766,  ..., -0.7891,  0.3379,  1.0156],\n",
       "          [-0.0413,  0.1953, -0.4941,  ..., -0.8086,  0.3027,  1.0391]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_v_proj': tensor([[[ 0.2412,  0.1875, -0.0952,  ..., -0.2158, -0.1484, -0.1074],\n",
       "          [ 0.2412,  0.1875, -0.0918,  ..., -0.2109, -0.1523, -0.1011],\n",
       "          [ 0.2412,  0.1807, -0.1143,  ..., -0.2295, -0.1436, -0.0850]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_attention': tensor([[[ 0.2559, -0.6523, -0.1299,  ...,  0.7305, -0.9453, -0.1396],\n",
       "          [ 0.2598, -0.6484, -0.1387,  ...,  0.7383, -0.9531, -0.1426],\n",
       "          [ 0.2598, -0.6328, -0.1436,  ...,  0.7461, -0.9375, -0.1357]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_output': tensor([[[-45.7500,  84.5000,  10.5625,  ..., -83.0000, -53.5000, -11.0625],\n",
       "          [-45.0000,  84.5000,   9.8750,  ..., -87.0000, -56.0000, -14.6250],\n",
       "          [-38.0000,  51.5000,   8.0625,  ..., -72.5000, -49.2500, -15.2500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_attn_norm': tensor([[[-0.0515,  0.0693,  0.0099,  ..., -0.0820, -0.0513, -0.0110],\n",
       "          [-0.0498,  0.0684,  0.0092,  ..., -0.0845, -0.0530, -0.0144],\n",
       "          [-0.0527,  0.0520,  0.0094,  ..., -0.0879, -0.0583, -0.0188]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_q_proj': tensor([[[-0.0233, -0.1709, -0.2480,  ..., -0.8281, -0.5195, -0.3906],\n",
       "          [-0.0164, -0.1709, -0.2461,  ..., -0.8281, -0.5195, -0.3887],\n",
       "          [-0.0189, -0.1660, -0.2354,  ..., -0.8281, -0.5391, -0.3887]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_k_proj': tensor([[[-0.0292, -0.2246,  0.8750,  ...,  0.2871, -0.3867, -0.0923],\n",
       "          [-0.0309, -0.2246,  0.8750,  ...,  0.3008, -0.3887, -0.0962],\n",
       "          [-0.0280, -0.2197,  0.8750,  ...,  0.2988, -0.4062, -0.0884]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_v_proj': tensor([[[-0.2500, -0.2188,  0.2471,  ..., -0.3301,  0.2041,  0.0894],\n",
       "          [-0.2324, -0.2334,  0.2422,  ..., -0.3340,  0.1992,  0.0854],\n",
       "          [-0.1992, -0.2109,  0.1875,  ..., -0.3359,  0.2051,  0.0339]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_attention': tensor([[[-1.2578e+00, -2.3145e-01, -5.3906e-01,  ..., -1.4531e+00,\n",
       "            6.6406e-01, -3.8719e-04],\n",
       "          [-1.2422e+00, -2.4414e-01, -5.4688e-01,  ..., -1.4844e+00,\n",
       "            6.7578e-01, -8.3618e-03],\n",
       "          [-1.2578e+00, -2.4316e-01, -5.6250e-01,  ..., -1.4844e+00,\n",
       "            6.5234e-01, -8.4839e-03]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_output': tensor([[[  21.0000,  115.0000,  -30.0000,  ..., -127.0000,  -20.7500,\n",
       "              2.7500],\n",
       "          [  22.7500,  115.5000,  -30.5000,  ..., -131.0000,  -22.5000,\n",
       "             -1.2500],\n",
       "          [  26.7500,   87.0000,  -28.7500,  ..., -115.0000,  -15.5000,\n",
       "              2.8750]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_attn_norm': tensor([[[ 0.0264,  0.1250, -0.0369,  ..., -0.1602, -0.0255,  0.0031],\n",
       "          [ 0.0281,  0.1230, -0.0369,  ..., -0.1621, -0.0272, -0.0014],\n",
       "          [ 0.0457,  0.1289, -0.0481,  ..., -0.1973, -0.0259,  0.0044]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_q_proj': tensor([[[-0.0625, -2.5938, -0.9219,  ...,  4.8438, -6.3750,  1.9766],\n",
       "          [-0.0742, -2.5938, -0.9219,  ...,  4.8125, -6.3750,  1.9844],\n",
       "          [-0.1748, -2.6562, -0.8945,  ...,  4.8125, -6.4375,  1.9297]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_k_proj': tensor([[[-0.1050,  0.2461, -0.6875,  ..., -2.9062, -0.1035, -2.3438],\n",
       "          [-0.1060,  0.2500, -0.6875,  ..., -2.9062, -0.0962, -2.3281],\n",
       "          [-0.0879,  0.2656, -0.7266,  ..., -2.8750, -0.0825, -2.3906]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_v_proj': tensor([[[ 0.0283, -0.2354, -0.8555,  ...,  0.1338, -0.5625,  0.3828],\n",
       "          [ 0.0515, -0.2471, -0.8398,  ...,  0.1348, -0.5195,  0.4395],\n",
       "          [-0.0143, -0.4941, -0.8594,  ...,  0.0698, -0.7070,  0.3438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_attention': tensor([[[-47.0000, -25.6250,  16.7500,  ...,  17.3750, -41.2500, -28.7500],\n",
       "          [-47.0000, -25.6250,  16.7500,  ...,  17.3750, -41.2500, -28.7500],\n",
       "          [-46.7500, -25.3750,  16.6250,  ...,  17.2500, -41.2500, -28.5000]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_output': tensor([[[ 23.5000, 113.0000, -21.5000,  ..., -65.0000,  54.0000,  -5.2500],\n",
       "          [ 25.0000, 114.5000, -21.2500,  ..., -69.0000,  54.7500,  -8.0000],\n",
       "          [ 26.7500,  83.0000, -18.7500,  ..., -48.5000,  62.2500,  -2.0000]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e4b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.6641,  1.5234,  0.2852,  ...,  0.1426, -0.2324,  0.5820],\n",
       "          [ 1.5234,  1.5938, -0.3984,  ..., -0.4883, -0.3789,  0.3828],\n",
       "          [ 0.6250,  2.2031, -1.1250,  ..., -0.0559,  0.4629,  0.3320]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[-0.4316,  1.1406,  0.4277,  ...,  3.0156, -5.6250,  8.5000],\n",
       "          [-0.9180,  1.0234,  0.4961,  ...,  2.3906, -5.2188,  8.3750],\n",
       "          [-0.4551,  1.2344,  0.3711,  ...,  3.7656, -7.5938,  9.0625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[-6.9824e-02, -7.3047e-01, -8.5156e-01,  ...,  1.1406e+00,\n",
       "            1.0312e+00,  9.4238e-02],\n",
       "          [-1.2305e-01, -1.2695e-01, -1.3086e-01,  ...,  1.2779e-04,\n",
       "            4.0234e-01,  1.1279e-01],\n",
       "          [ 1.4258e-01, -1.4465e-02,  1.5820e-01,  ..., -1.7188e-01,\n",
       "           -6.3672e-01,  1.7090e-01]]], device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[\"layer_0_wqkv\"].split([3584, 512, 512], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f34499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q torch.Size([1, 3, 3584]) k torch.Size([1, 3, 512]) v torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "q, k, v = activations[\"layer_0_wqkv\"].split([3584, 512, 512], dim=-1)\n",
    "print(\"q\", q.shape, \"k\", k.shape, \"v\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "467f9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_custom = activations['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82f6e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings dtype:\", model.tok_embeddings.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c42e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "BFloat16 did not match Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_hf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: BFloat16 did not match Float"
     ]
    }
   ],
   "source": [
    "torch.allclose(embeddings_custom, embeddings_hf, rtol=1e-5, atol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f8aa972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_custom.dtype, embeddings_hf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d329da02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ef3fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pos = torch.tensor([prompt_length], device=device, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32dce563",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(next_token, input_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token, next_prob = sample(logits, temperature=0, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50590cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[220]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token.shape\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c0250b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next_token[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a6d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
