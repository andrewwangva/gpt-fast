{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d77d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25be6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25eaa7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e115d796f89455eafea30da132312c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9958d813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6152e-03, -2.1606e-02, -8.9111e-03,  ..., -6.9580e-03,\n",
       "          3.6621e-02, -1.3245e-02],\n",
       "        [ 4.4922e-02,  4.7363e-02,  1.6602e-02,  ...,  7.2021e-03,\n",
       "          2.9564e-04, -1.6235e-02],\n",
       "        [-1.6724e-02, -6.4392e-03,  1.4400e-04,  ...,  1.0824e-04,\n",
       "         -2.1973e-02,  2.3804e-03],\n",
       "        ...,\n",
       "        [-1.1755e-37,  1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37,  1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37, -1.1755e-37,  ...,  1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_tokens.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd023eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850a6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = inputs[\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1ecbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646,   9707,   1879]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a007c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(28):\n",
    "    print(model.layers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 3, 3584])\n",
      "Layer 0 shape: torch.Size([3, 3584])\n",
      "Layer 15 shape: torch.Size([3, 3584])\n",
      "Layer 27 shape: torch.Size([3, 3584])\n",
      "\n",
      "All captured activations:\n",
      "embeddings: torch.Size([1, 3, 3584])\n",
      "layer_0: torch.Size([1, 3, 3584])\n",
      "layer_1: torch.Size([1, 3, 3584])\n",
      "layer_2: torch.Size([1, 3, 3584])\n",
      "layer_3: torch.Size([1, 3, 3584])\n",
      "layer_4: torch.Size([1, 3, 3584])\n",
      "layer_5: torch.Size([1, 3, 3584])\n",
      "layer_6: torch.Size([1, 3, 3584])\n",
      "layer_7: torch.Size([1, 3, 3584])\n",
      "layer_8: torch.Size([1, 3, 3584])\n",
      "layer_9: torch.Size([1, 3, 3584])\n",
      "layer_10: torch.Size([1, 3, 3584])\n",
      "layer_11: torch.Size([1, 3, 3584])\n",
      "layer_12: torch.Size([1, 3, 3584])\n",
      "layer_13: torch.Size([1, 3, 3584])\n",
      "layer_14: torch.Size([1, 3, 3584])\n",
      "layer_15: torch.Size([1, 3, 3584])\n",
      "layer_16: torch.Size([1, 3, 3584])\n",
      "layer_17: torch.Size([1, 3, 3584])\n",
      "layer_18: torch.Size([1, 3, 3584])\n",
      "layer_19: torch.Size([1, 3, 3584])\n",
      "layer_20: torch.Size([1, 3, 3584])\n",
      "layer_21: torch.Size([1, 3, 3584])\n",
      "layer_22: torch.Size([1, 3, 3584])\n",
      "layer_23: torch.Size([1, 3, 3584])\n",
      "layer_24: torch.Size([1, 3, 3584])\n",
      "layer_25: torch.Size([1, 3, 3584])\n",
      "layer_26: torch.Size([1, 3, 3584])\n",
      "layer_27: torch.Size([1, 3, 3584])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        #print(name, output, type(output))\n",
    "        if(\"layer\" in name):\n",
    "            activations[name] = output[0].detach()  # For transformer layers, output is a tuple (last_hidden_state, past_key_values)\n",
    "        else:\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks for all transformer layers\n",
    "# DeepSeek-R1-Distill-Qwen-7B has 32 layers (layers 0-31)\n",
    "for i in range(28):\n",
    "    model.layers[i].register_forward_hook(get_activation(f'layer_{i}'))\n",
    "\n",
    "\"\"\"\n",
    "# You can also register hooks for specific components within layers\n",
    "# For example, attention and MLP outputs:\n",
    "for i in range(28):\n",
    "    model.layers[i].self_attn.register_forward_hook(get_activation(f'layer_{i}_attention'))\n",
    "    model.layers[i].mlp.register_forward_hook(get_activation(f'layer_{i}_mlp'))\n",
    "\"\"\"\n",
    "# Register hook for embeddings\n",
    "model.embed_tokens.register_forward_hook(get_activation('embeddings'))\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_ids=inputs)\n",
    "\n",
    "# Access activations\n",
    "print(f\"Embeddings shape: {activations['embeddings'].shape}\")\n",
    "print(f\"Layer 0 shape: {activations['layer_0'][0].shape}\")  # Note: output is tuple, take first element\n",
    "print(f\"Layer 15 shape: {activations['layer_15'][0].shape}\")\n",
    "print(f\"Layer 27 shape: {activations['layer_27'][0].shape}\")\n",
    "\n",
    "# Print all available activation keys\n",
    "print(\"\\nAll captured activations:\")\n",
    "for key in activations.keys():\n",
    "    if isinstance(activations[key], tuple):\n",
    "        print(f\"{key}: {activations[key][0].shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {activations[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embeddings', 'layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24', 'layer_25', 'layer_26', 'layer_27'])\n"
     ]
    }
   ],
   "source": [
    "print(activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85805087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[[-0.0006,  0.0005, -0.0049,  ..., -0.0001, -0.0004, -0.0010],\n",
      "         [-0.0060, -0.0042,  0.0084,  ...,  0.0510,  0.0008, -0.0086],\n",
      "         [-0.0225, -0.0293,  0.0107,  ..., -0.0100,  0.0104, -0.0571]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings:\", activations['embeddings'], activations['embeddings'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54579ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: tensor([[[-1.6078,  1.7398,  0.9056,  ...,  2.0966, -0.3973, -0.7354],\n",
      "         [-0.4516,  1.4271,  0.0635,  ...,  0.9967,  0.3587, -1.0365],\n",
      "         [ 0.3759,  0.4770,  0.5341,  ...,  0.9871,  0.5478,  0.1407]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 0:\", activations['layer_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631761da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "import torch._inductor.config\n",
    "from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523f091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "from tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e466fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(\"checkpoints/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/model.pth\")\n",
    "tokenizer_path = checkpoint_path.parent / \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6ae734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87457817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _load_model(checkpoint_path, device, precision, use_tp):\n",
    "    use_cuda = 'cuda' in device\n",
    "    with torch.device('meta'):\n",
    "        model = Transformer.from_name(checkpoint_path.parent.name)\n",
    "\n",
    "    if \"int8\" in str(checkpoint_path):\n",
    "        print(\"Using int8 weight-only quantization!\")\n",
    "        from quantize import WeightOnlyInt8QuantHandler\n",
    "        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n",
    "        model = simple_quantizer.convert_for_runtime()\n",
    "\n",
    "    if \"int4\" in str(checkpoint_path):\n",
    "        print(\"Using int4 weight-only quantization!\")\n",
    "        path_comps = checkpoint_path.name.split(\".\")\n",
    "        groupsize = int(path_comps[-2][1:])\n",
    "        from quantize import WeightOnlyInt4QuantHandler\n",
    "        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n",
    "        model = simple_quantizer.convert_for_runtime()\n",
    "\n",
    "    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n",
    "    if \"model\" in checkpoint and \"stories\" in str(checkpoint_path):\n",
    "        checkpoint = checkpoint[\"model\"]\n",
    "    model.load_state_dict(checkpoint, assign=True)\n",
    "\n",
    "    if use_tp:\n",
    "        from tp import apply_tp\n",
    "        print(\"Applying tensor parallel to model ...\")\n",
    "        apply_tp(model)\n",
    "\n",
    "    model = model.to(device=device, dtype=precision)\n",
    "    return model.eval()\n",
    "\n",
    "device = \"cuda\"\n",
    "precision = torch.bfloat16\n",
    "use_tp = False\n",
    "\n",
    "model = _load_model(checkpoint_path, device, precision, use_tp)\n",
    "with torch.device(device):\n",
    "    model.setup_caches(max_batch_size=1, max_seq_length=212)\n",
    "model.causal_mask.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e76ee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6152e-03, -2.1606e-02, -8.9111e-03,  ..., -6.9580e-03,\n",
       "          3.6621e-02, -1.3245e-02],\n",
       "        [ 4.4922e-02,  4.7363e-02,  1.6602e-02,  ...,  7.2021e-03,\n",
       "          2.9564e-04, -1.6235e-02],\n",
       "        [-1.6724e-02, -6.4392e-03,  1.4400e-04,  ...,  1.0824e-04,\n",
       "         -2.1973e-02,  2.3804e-03],\n",
       "        ...,\n",
       "        [-1.1755e-37,  1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37,  1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37, -1.1755e-37,  ...,  1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tok_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6e34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokenizer, string, bos=True, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(string)\n",
    "    if bos:\n",
    "        tokens = [tokenizer.bos_id()] + tokens\n",
    "    return torch.tensor(tokens, dtype=torch.int, device=device)\n",
    "encoded = encode_tokens(tokenizer, \"Hello World\", bos=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a3ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = encoded.view(1, -1).repeat(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229171d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151646,   9707,   4337]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0bdf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = prompt.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b021e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pos = torch.arange(0, prompt_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b04520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b1b49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations[name] = output[0].detach()\n",
    "        else:\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.tok_embeddings.register_forward_hook(get_activation('embeddings'))\n",
    "for i, block in enumerate(model.layers):\n",
    "    block.register_forward_hook(get_activation(f'layer_{i}_output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c50c984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes torch.Size([1, 3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n",
    "    q = torch.empty_like(probs_sort).exponential_(1)\n",
    "    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "\n",
    "def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
    "    logits = logits / max(temperature, 1e-5)\n",
    "\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
    "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
    "    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n",
    "    idx_next = multinomial_sample_one_no_sync(probs)\n",
    "    return idx_next, probs\n",
    "def prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n",
    "    # input_pos: [B, S]\n",
    "    logits = model(x, input_pos)\n",
    "    return sample(logits, **sampling_kwargs)[0]\n",
    "print(\"shapes\", prompt.shape, input_pos.shape)\n",
    "next_token = prefill(model, prompt, input_pos, temperature=0, top_k=1).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d329da02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ef3fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pos = torch.tensor([prompt_length], device=device, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32dce563",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(next_token, input_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d6aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token, next_prob = sample(logits, temperature=0, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e50590cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c0250b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next_token[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "680b1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.0006,  0.0005, -0.0049,  ..., -0.0001, -0.0004, -0.0010],\n",
       "          [-0.0060, -0.0042,  0.0084,  ...,  0.0510,  0.0008, -0.0086],\n",
       "          [ 0.0299,  0.0116,  0.0133,  ..., -0.0177, -0.0156, -0.0669]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_0_output': tensor([[[-1.3750,  1.4297,  0.7773,  ...,  1.8672, -0.3555, -0.6875],\n",
       "          [-0.8516,  1.2031,  0.6094,  ...,  1.3359,  0.3320, -0.8398],\n",
       "          [-0.1143,  0.3535,  0.2148,  ...,  1.0625,  0.6680, -0.4160]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_1_output': tensor([[[-1.1641,  1.1641,  0.3398,  ...,  1.5859,  0.0703, -0.2891],\n",
       "          [-0.6992,  0.6523,  0.2461,  ...,  1.0781,  0.6641, -0.3906],\n",
       "          [ 0.1206,  0.0645, -0.1699,  ...,  0.5703,  0.9258, -0.0337]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_2_output': tensor([[[-1.4375,  1.0938, -0.6953,  ...,  2.4062, -0.7148, -0.8594],\n",
       "          [-0.8047,  0.6797, -0.6016,  ...,  1.4062,  0.0156, -0.8242],\n",
       "          [ 0.1069,  0.1387, -0.9141,  ...,  0.8047,  0.4941, -0.2041]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_3_output': tensor([[[-18.8750, 151.0000,   3.1250,  ..., -76.0000,  -3.1719,  12.3125],\n",
       "          [-14.6875, 126.5000,   1.9062,  ..., -66.5000,  -5.4062,   6.1250],\n",
       "          [ -8.4375,  82.0000,   0.5430,  ..., -48.2500,  -2.5625,   3.2812]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_4_output': tensor([[[-20.5000, 135.0000,  -2.6875,  ..., -79.5000,   1.2188,  13.5000],\n",
       "          [-16.3750, 111.0000,  -3.9375,  ..., -70.0000,  -1.0625,   7.3125],\n",
       "          [-10.2500,  66.5000,  -5.3750,  ..., -51.5000,   1.8594,   4.4375]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_5_output': tensor([[[-21.5000, 132.0000,  -3.1250,  ..., -78.5000,   0.5977,  13.3125],\n",
       "          [-17.3750, 108.0000,  -4.4062,  ..., -69.0000,  -1.7031,   7.1250],\n",
       "          [-11.2500,  63.2500,  -5.9688,  ..., -50.5000,   1.1875,   4.2812]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_6_output': tensor([[[-21.0000, 130.0000,  -1.1172,  ..., -78.5000,   1.4531,  13.6250],\n",
       "          [-16.8750, 106.5000,  -2.4219,  ..., -69.0000,  -0.8281,   7.4375],\n",
       "          [-10.8750,  61.5000,  -4.0000,  ..., -50.7500,   2.1406,   4.6250]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_7_output': tensor([[[-21.2500, 129.0000,  -0.8984,  ..., -79.5000,   1.7344,  12.8750],\n",
       "          [-17.1250, 106.0000,  -2.2188,  ..., -70.0000,  -0.5586,   6.6875],\n",
       "          [-11.1250,  61.0000,  -3.8125,  ..., -52.0000,   2.3750,   3.9062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_8_output': tensor([[[-22.0000, 130.0000,  -1.2891,  ..., -79.5000,   1.6719,  12.2500],\n",
       "          [-17.8750, 106.5000,  -2.5938,  ..., -70.0000,  -0.6250,   6.0938],\n",
       "          [-11.8750,  61.7500,  -4.1562,  ..., -52.2500,   2.2969,   3.3125]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_9_output': tensor([[[-23.0000, 129.0000,  -4.0312,  ..., -78.5000,   3.2031,  10.0000],\n",
       "          [-18.8750, 105.5000,  -5.2500,  ..., -69.0000,   0.9492,   3.8281],\n",
       "          [-13.0000,  60.5000,  -6.6875,  ..., -51.2500,   4.0312,   0.8750]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_10_output': tensor([[[-23.0000, 129.0000,  -5.4375,  ..., -78.0000,   2.8750,  10.1250],\n",
       "          [-18.8750, 105.0000,  -6.6250,  ..., -68.5000,   0.6289,   3.9531],\n",
       "          [-13.1250,  60.0000,  -8.0625,  ..., -50.7500,   3.7188,   0.9922]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_11_output': tensor([[[-23.2500, 129.0000,  -5.5625,  ..., -77.5000,   2.8594,  10.3750],\n",
       "          [-19.1250, 105.5000,  -6.7500,  ..., -68.0000,   0.6172,   4.1875],\n",
       "          [-13.3125,  60.2500,  -8.1875,  ..., -50.0000,   3.6719,   1.2266]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_12_output': tensor([[[-2.4250e+01,  1.2800e+02, -5.1250e+00,  ..., -7.7000e+01,\n",
       "            2.2656e+00,  1.0375e+01],\n",
       "          [-2.0125e+01,  1.0500e+02, -6.2812e+00,  ..., -6.7500e+01,\n",
       "            3.1250e-02,  4.2500e+00],\n",
       "          [-1.4250e+01,  5.9750e+01, -7.6875e+00,  ..., -4.9750e+01,\n",
       "            3.0469e+00,  1.2656e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_13_output': tensor([[[-2.4375e+01,  1.2750e+02, -5.6250e+00,  ..., -7.6500e+01,\n",
       "            2.1406e+00,  1.1188e+01],\n",
       "          [-2.0250e+01,  1.0450e+02, -6.7500e+00,  ..., -6.7000e+01,\n",
       "           -1.0059e-01,  5.0312e+00],\n",
       "          [-1.4438e+01,  5.9250e+01, -8.1250e+00,  ..., -4.9500e+01,\n",
       "            2.8750e+00,  2.0938e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_14_output': tensor([[[-23.8750, 127.0000,  -5.9062,  ..., -76.0000,   1.9141,  11.0000],\n",
       "          [-19.7500, 104.0000,  -7.0312,  ..., -66.5000,  -0.3242,   4.8438],\n",
       "          [-13.9375,  59.0000,  -8.3750,  ..., -49.5000,   2.6406,   1.9062]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_15_output': tensor([[[-2.4000e+01,  1.2650e+02, -5.3125e+00,  ..., -7.5500e+01,\n",
       "            2.1719e+00,  1.1062e+01],\n",
       "          [-1.9875e+01,  1.0350e+02, -6.4375e+00,  ..., -6.6000e+01,\n",
       "           -5.9326e-02,  4.9062e+00],\n",
       "          [-1.4062e+01,  5.8750e+01, -7.7500e+00,  ..., -4.9000e+01,\n",
       "            2.9062e+00,  1.9375e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_16_output': tensor([[[-23.8750, 126.0000,  -4.7188,  ..., -75.0000,   2.0469,  10.9375],\n",
       "          [-19.7500, 103.0000,  -5.8438,  ..., -65.5000,  -0.1982,   4.7500],\n",
       "          [-14.0000,  58.5000,  -7.1250,  ..., -48.7500,   2.7500,   1.7812]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_17_output': tensor([[[-23.7500, 126.0000,  -5.1562,  ..., -75.0000,   2.5469,  11.4375],\n",
       "          [-19.6250, 103.0000,  -6.2812,  ..., -65.5000,   0.2930,   5.2188],\n",
       "          [-13.9375,  58.5000,  -7.5625,  ..., -49.2500,   3.2188,   2.2344]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_18_output': tensor([[[-2.2875e+01,  1.2550e+02, -4.3438e+00,  ..., -7.4500e+01,\n",
       "            2.2656e+00,  1.1625e+01],\n",
       "          [-1.8750e+01,  1.0250e+02, -5.5000e+00,  ..., -6.5000e+01,\n",
       "            5.8594e-03,  5.3750e+00],\n",
       "          [-1.3125e+01,  5.8250e+01, -6.9375e+00,  ..., -4.9000e+01,\n",
       "            2.9688e+00,  2.3125e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_19_output': tensor([[[-23.0000, 124.0000,  -2.9375,  ..., -74.5000,   1.8828,  11.8125],\n",
       "          [-18.8750, 101.0000,  -4.1562,  ..., -65.0000,  -0.3848,   5.6250],\n",
       "          [-13.3750,  56.7500,  -5.6875,  ..., -48.7500,   2.5312,   2.5938]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_20_output': tensor([[[-23.3750, 122.5000,  -4.0625,  ..., -74.5000,   0.4609,  11.6250],\n",
       "          [-19.2500,  99.5000,  -5.2500,  ..., -65.0000,  -1.7812,   5.4688],\n",
       "          [-13.7500,  55.5000,  -6.7188,  ..., -48.5000,   1.1562,   2.4219]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_21_output': tensor([[[-2.4000e+01,  1.2100e+02, -4.0938e+00,  ..., -7.5000e+01,\n",
       "            6.2500e-02,  9.1875e+00],\n",
       "          [-1.9875e+01,  9.8000e+01, -5.2500e+00,  ..., -6.5500e+01,\n",
       "           -2.2188e+00,  3.0312e+00],\n",
       "          [-1.4375e+01,  5.4250e+01, -6.7188e+00,  ..., -4.9000e+01,\n",
       "            6.6797e-01, -6.2500e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_22_output': tensor([[[-26.3750, 119.0000,  -4.2812,  ..., -75.5000,  -2.1250,   8.0000],\n",
       "          [-22.2500,  96.0000,  -5.4062,  ..., -66.0000,  -4.4062,   1.8125],\n",
       "          [-17.0000,  52.5000,  -6.8750,  ..., -50.0000,  -1.5156,  -1.3906]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_23_output': tensor([[[-28.6250, 117.5000,  -3.9375,  ..., -76.0000,  -6.3125,   7.1250],\n",
       "          [-24.5000,  94.5000,  -5.1250,  ..., -66.5000,  -8.5625,   0.8906],\n",
       "          [-19.3750,  51.2500,  -6.7188,  ..., -50.0000,  -5.6250,  -2.2344]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_24_output': tensor([[[-30.1250, 112.0000,  -1.4453,  ..., -75.0000, -10.5000,   2.7031],\n",
       "          [-26.0000,  89.5000,  -2.7656,  ..., -66.0000, -12.6875,  -3.3750],\n",
       "          [-20.8750,  46.2500,  -4.5000,  ..., -49.0000,  -9.6250,  -6.2500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_25_output': tensor([[[-48.0000,  96.0000,  11.2500,  ..., -92.0000, -56.2500, -10.8125],\n",
       "          [-42.0000,  74.5000,   9.2500,  ..., -81.5000, -55.0000, -16.2500],\n",
       "          [-32.7500,  33.5000,   5.5938,  ..., -60.2500, -43.2500, -16.7500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_26_output': tensor([[[  20.2500,  125.0000,  -30.5000,  ..., -136.0000,  -23.5000,\n",
       "              1.2500],\n",
       "          [  25.2500,  106.5000,  -30.0000,  ..., -125.5000,  -21.2500,\n",
       "             -1.4375],\n",
       "          [  30.5000,   73.5000,  -28.0000,  ..., -102.0000,   -8.7500,\n",
       "              5.0000]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'layer_27_output': tensor([[[ 23.5000, 124.0000, -22.2500,  ..., -75.0000,  52.5000,  -6.2500],\n",
       "          [ 26.7500, 105.0000, -20.0000,  ..., -63.7500,  55.5000,  -7.6250],\n",
       "          [ 24.0000,  65.0000, -18.7500,  ..., -24.2500,  76.0000,   1.7500]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a6d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
